\chapter{Discussion}


\section{ Important considerations with computational clustering methods }

Throughout my thesis I have considered methods of computationally analysing flow cytometry and genetic datasets.
The key motivation was that these methods would be more efficient, consistent, objective and better at dealing with uncertainty, than manual analysis of the data.
In the case of flow data, automating gating can also provide the added benefit of formalising manual gating, which remains to this day a very subjective process.
The hope was that these methods would improve on the reproducibility of results, thus enabling more powerful association testing.
Assessing how well computational methods perform is a matter of debate and it is safe to say very much data-dependent.
However, I will attempt here to give an overview of the important practical and theoretical considerations of these methods, in the context of my thesis.

\paragraph{Memory usage and running time}
Two crucial practical properties are their memory usage and running time.
While the applications that have been studied in this thesis don’t require real-time analysis, there are still some practical limitations on the amount of memory and compute time demands.
It is a well known fact in computer science that memory can often be traded for running time.
For example, methods which rely on global knowledge of the data, such as computation of the complete pairwise distance matrix,
have a large initial memory footprint ( data matrix of size $\frac{N \times (N-1)}{2}$ ), but only require one pass of the data.
A solution to the clustering solution is reached within just a few computational steps, by for instance,
selecting a distance cut threshold on the dendrogram generated from hierarchical clustering to define K clusters. 
On the other hand, methods like K-means necessitate a much smaller memory footprint
since they only compute the distance of every point to the K cluster means
( data matrix of size $N \times K$), which is usually a much smaller number than the total number of points,
but on the other hand, several updates of the matrix are required until the cluster centers are fixed.
The choice of which method to apply is very much data dependent.
In ungated flow cytometry, data matrices  contain over a million rows, so the complete pairwise distance matrix is too large to fit in memory,
making these type of methods impractical.
I found that distance matrix computation was only feasible in subsets of the order of $10,000$,
like for example the CD4+ lymphocyte subset, or data downsampled using the SPADE.
The data downsampling as performed by SPADE, relies on 
first estimating the local density at each point
and then preferentially thinning the data in regions of high density to even out the density 
across the whole sample.
The local density estimation step is computationally intensive as it needs to consider the distance from
a single point to all other points in the sample to find the number of neighbours.
I found this step could be greatly sped up using \gls{ANN} which uses the K dimensional trees (KD tree) lookup method.
KD-trees are a space-partitioning data structure for organising points in a k-dimensional space which makes for an efficient
way of storing a high-dimensional dataset to lookup proximal datapoints.
It can also be used for approximation to reduce the number of datapoints which need to be considered.
This approach can also be applied to mixture models \citep{McLachlan:2004uw}, but I haven’t had the opportunity of trying this. 

\paragraph{Consistency and accuracy}
Consistency can be defined as how often does a method return a similar result.
This obviously depends on how much the data has changed.
If a method is consistent, one would expect that small perturbations to the data should lead to small changes in the clustering outcome.
However, algorithms which rely on initialisation using random starting positions like K-means may reach different clustering solutions, even on the same data!
When running K-means I therefore had to pick the initial clusters or run it with multiple restarts.
Over all runs of K-means the solution which returns the lowest within-sum-of-squares is picked.
The accuracy of a method is defined based on how frequently the methods assigns the correct label.
Hence it relies on the existence of a test dataset, typically labelled using manual analysis or some other method.
In Chapter 2, accuracy was assessed by comparing the cluster proportions and means with those obtained from manual gating.
In Chapter 2, the accuracy was determined in terms of RSS of the pSTAT5 response.
In Chapter 4, I used qPCR labelled data to assess the prediction accuracy of the \gls{knn} classifier.
However, labelled data may not always be available especially in the case of flow cytometry data.
Also, in the context of flow cytometry, even when labelled data is available, this approach may not always be ideal,
as it is merely comparing the relative agreement between methods rather than the objective truth.
A sometimes more useful alternative is instead to assess the prediction accuracy with clinical outcome, case-control status or genotype.
This is usually implicitly measured when doing an association test.
The consistency-accuracy tradeoff is analogous to the variance-bias tradeoff in statistics.
Consistency and accuracy are typically measured on simulated data.

\paragraph{Interpretability}
While a method might be accurate and consistent, it may be difficult to interpret the results, much like a black box.
Making a model more flexible by adding parameters can obfuscate the relationship between the input parameters and the clustering output.
Random forests and neural networks are example of methods from which it is difficult to extract an interpretable model to justify the result.
This is an issue because
%generally people (and especially inquisitive minds like scientists) are not comfortable with applying methods they don’t understand.
as part of the iterative process of knowledge discovery, its important to understand what combination of features make objects distinguishable.
%Since fully automated methods are hard to assess.
Sometimes the additional information which is available from previous studies can be used to guide the clustering.


%\paragraph{Parallelisation}
%Provided the data can be split into independent chunks, then this feature can be exploited with parallelisation.
%By make astute use of parallelisation, one can marry local and global methods to make efficient use of the computational facilities
%available such as compute farms of many nodes with limited memory and processing power.
%In such a scenario, each compute node can work on subset of the data to compute intermediate results which can then be combined.
%In the context of flow cytometry however, samples were not split but analysed individually on each compute node.
%In the future, it may be possible to exploit the hierarchical nature of the gating to conduct parallel analysis of independent subsets.

\section{Supervised clustering}

Often clusters are not easily identifiable because of noise or insufficient number of markers.
%and become harder to call when the number of dimensions increases (they are not linearly separable in all dimensions).
Knowing certain properties of the clustering can help, especially in identifying smaller groups.


In \cref{chapter:kir}, I used prior information about the copy number of the \gene{KIR3DL1}/\gene{KIR3DS1} genes, obtained from qPCR,
in order to identify clusters in the SNP dataset.
The \gls{knn} was used to classify points unlabelled points (without qPCR data) from the vote of their K nearest labelled neighbours.
Euclidean distance was used to define the distance.
The optimal value of K was selected by minimsing the \gls{LOOCV}.
Another source of prior information in actually labelling the samples with qPCR,
could have been to use expected copy number group frequencies obtained from previous studies 
\citet{Jiang:2012cf}, in order to identify copy number groups in the qPCR data.

In the flow data, the manual gates contribute prior information to guide the automatic gating.  

The established way in FlowJo is to use the gate coordinates and apply the same
gate coordinates across samples.
Manual gates can also provide initial starting parameters to a clustering algorithm,
by calculating the mean and the covariance of the gated populations.
%For example, the labelling in one sample to the mean and covariance of an ellipse which can then be applied to another sample.
Using an EM style algorithm, the parameters of the ellipse can then be influenced by the data.
I have let the mean of the ellipse be influenced by the data while the covariance was set as fixed.
The Mahalanobis distance then allows us to go from a mean and covariance of an ellipse to a classification by defining a threshold
above which points are excluded from the ellipse.

If the manual gating is done in several samples then 
they can be combined to define the priors in the mixture model,
so to guide the parameter estimation.
as is done in flowClust,
If several samples have been manually gated then these can be incorporated in the definition of the prior on the parameters of the mixture model.
%FlowClust as opposed to Mclust offers the option of defining per component priors.
%This threshold can be defined by using a chi squared distribution in the one dimensional case.

One outstanding problem remains with supervised clustering however,
how to account for unlabelled data, or in the context of flow cytometry,
nuisance clusters which are not part of the gating?
While in manual gating these points are discarded, in automatic gating all points need
to be accounted for and consequently these points influence the gating.
One solution could be to define a background cluster but this is unlikely to work
for multimodal distributions.  Another solution could be to allow for the creation of a
large number of clusters.


Supervised clustering can be really beneficial in trying to identify rare subsets.
This is because rare subsets, by definition, may not always be visible in all samples.
%Futhermore another concern with clustering and statistics in general is distinguising rare subsets from outliers.


\section{ Distinguishing between outliers and rare subsets }

Outliers have an important influence on the mean of the data.
They lie far from cluster means, usually in a sparsely inhabited region of space.
Thus clustering methods and statistical tests which rely on mean or covariance estimation are sensitive to outliers.
Certain outliers may be clearly identifiable because their values lie at the extreme of the instrument range,
far from the rest of the data.
%Very large intensity values are usually considered as noise, but low intensity values are harded to call with certainty.
Others which lie within a more plausible range of values are harder to identify.
Also in higher dimensions, data points can be outliers without being outliers in any single dimension.
Generally in order to detect outliers, large sample sizes are required.
Reducing the dimensionality makes outliers easier to identify.
%There is a soft approach of dealing with outliers and a hard approach.

Flow cytometry datasets contain millions of events and many events are discarded as part of the gating process.
Some of them are real biological clusters which are ignored because they are not part of the cell populations under study:
for example the first gate drawn is usually on the lymphocytes whereas the monocytes and granulocytes are often ignored.
This this in part because they are no markers included in the experiment defined on these cell populations which would allow any further division.
Other events are discarded because they are outliers. These outliers occur commonly in flow cytometry datasets caused by debris and cells clumping together.
They can be found at the extremes of the value range on a given channel, but they may also be found with intermediate values and these are harder to spot and require large samples sizes to confidently call as outliers.
Model-based methods can account for these by defining a background mixture with a covariance defined on the entire sample which essentially mops up all points
which are not assigned with high posterior weight to any of the known components.
Mclust allows to define a noise component for that purpose.
On the other hand, flowClust  has an outliyingness parameter which is inversely proportional to the Mahalanobis distance from a point to a cluster.
%g:

%\[
  %u_{ig} = \frac{v+p}{v+(\boldsymbol{x_i}-\boldsymbol{\mu_g})^{\top}\boldsymbol{\Sigma}_{g}^{-1}(\boldsymbol{x_i}-\boldsymbol{\mu_g})}
%\]

Another method which does not rely on a background component is to exclude low density points from belonging to any of the components.
A metric for detecting outliers is the Mahalanobis distance, large distances imply a point is far from a cluster center:
\[
(\boldsymbol{x_i}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x_i}-\boldsymbol{\mu})
\]
where $\boldsymbol{x_i}$ is the point, and $\boldsymbol{\mu}$ is the cluster mean and $\boldsymbol{\Sigma}$ is the cluster covariance.

One issue however is that the outliers influences and greatly inflate the covariance matrix which is used in calculating the Mahalanobis distance.
To address this, R packages like robustbase,
use leave-one-out methods to identify outliers which have high leverage on the covariance estimation.
Another outlier metric, commonly used in linear regression is Cook's distance,
which returns the leverage of every data point $i$  on the estimation of $\hat Y$ when the point is left out:
\[
D_i = \frac{ \sum_{j=1}^n (\hat Y_j\ - \hat Y_{j(i)})^2 }{p \ \mathrm{MSE}}
\]

In genetics, outliers may be harder to spot because of smaller sample sizes which also implies that we are also generally more reluctant to exclude samples.
In \Cref{chapter:KIR}, a whole qPCR plate was excluded because the $\Delta$Ct distribution didn’t align well even after normalisation.
Later I found however that qPCR samples may possibly be scored independently of where they lie in relation to other samples by considering their background Ct median and spread.
This score could have been used to downweight these samples in the mixture model clustering which was used to assign them to copy number groups.

As well as techniques for visualising multiple dimensions.
Tools are being developed for visualisation of summary statistics in mutliple samples in order to spot outliers.
Boxplots are the typical visualisation tool for viewing univariate intensity data but in flow cytometry where we typically have multivariate data,
SPICE presents an overview using pie charts of multiple samples \citep{Roederer:2011hy}.
%An alternative visual representation are radarplots

They are however obvious outliers and less obvious outliers, which is why I believe a confidence score is appropriate in calling outliers.
While certain outliers can be confidently called independently because their values are at the extreme of the range of the instrument,
others are less clear and should not be discarded completely because in light of new data they may be assigned to existing clusters or
to new clusters.

In particular distinguishing between rare subsets and outliers is hard as illustrated in \Cref{chapter:kir},
where the 3-0 copy number group is rare and could have been regarded as outliers had not we not had strong prior evidence supporting its existence
from previous studies.
However having prior evidence supporting the existence of a cluster is not sufficient to identify the cluster.
Priors usually determine expected proportions but give little information about the exact position and shape of the cluster, which are usually experiment
specific.
This can be due to the stability of the biological sample but also the stability of the instrumentation and sample preparation.
In genotyping calling a genotype based on only a few samples is sometimes possible, since DNA is a very stable molecule and SNP arrays are highly standardised
\citep{Di:2005uj,Giannoulatou:2008ty}.
In flow cytometry, the MFI of cell populations is generally not directly comparable across staining panels but instead clusters can be defined in relation to one another.


\section{Prioritising normalisation or clustering}
%\section{To move data or to move gate: normalisation vs clustering}

%Sometimes it is better not to normalise and normalisation may hide the true biological effect.
%The question really boils down to whether we should normalise the samples or cluster each sample separately.
%Normalisation is effectively analogous to meta-clustering since we are attempting to match cell populations across samples.  
%Normalisation facilitates matching clusters across datasets but in doing can remove meaningful biological variation.  
%Theoretically if the data was perfectly aligned across days then gates should not be moved.  
%Threshold gates should not move unless they are relative to some internal control.
%The cell phenotypes which are usually measured are the mean intensity of a particular channel, or the percentage of cells.
%Depending on the shape of the distribution these can be more or less sensitive to the position of the gate.
%Normalisation removes unwanted experimental variation to make data comparable even when collected on different days,
%proccessed with different protocols or analysed with different instruments.
%However distinguishing between what is unwanted and what is meaningful variation relies ultimately on a bias-prone judgement call.
%As we have seen, different normalisation approaches make different assumptions about what is unwanted variation and the shape of the data.
%The actual choice of normalisation depends on the characteristics of the data we wish to compare.
%We have only considered the univariate density function here but identification can be extended to the multivariate case.
%Next let's consider normalisation of multivariate distributions.  
%The purpose of principal component correction is to remove unwanted correlation between samples.  
%When applying a linear transform to multivariate data the correlation between the dimensions is preserved since the multiplicative factors cancel out.
%However the covariance changes.
%Care must be taken when applying a non-linear transform.  
%The samples are related and we wish to exploit this structure without forcing data to be perfectly comparable.
%A less biased approach is to allow for different cluster location and shapes across datasets and instead incorporate the clustering in some sort of hierarchical framework.  
%Normalisation or meta-clustering is a necessary step when data is pooled across studies.  
%One issue with normalisation based on peak alignment is that the level of there is more noise variation than biological variation between samples so that in certain samples,
%so much so that in certain samples the peaks are only identifiable after preliminary gating of lymphocytes.  

Normalisation is a two-part process, the first part involves matching across samples,
the second part involves transforming the data so that the data is directly comparable across samples.

In certain situations,
%If matching across samples involves find similar features
the first part of normalisation is similar to clustering and can rely on related algorithms.
For example normalisation by peak-alignment can use clustering of univariate data to identify peaks (with known k).
This is the method I used in \Cref{chapter:il2ra} when gating bead data and the transform I defined to align the peaks of the bead data,
was later applied to the biological data to do bead-normalisation.
When K is unknown a sliding window approach can be used to estimate the number of modes peaks.
The highest peaks can then be selected across samples in the hope that these are the same across samples.
Here normalisation is comparable to mode hunting in the density function.
The number of peaks returned is influenced by the span of the sliding window.
A large window span will tend to oversmooth the data, leading to fewer peaks while
a smaller window will call more peaks, but increases the chances of picking up spurious peaks.
Hence the number of clusters is controlled by the window span parameter, although the relationship between the parameter and the number of clusters is data dependent.

While normalisation is typically applied to univariate data whereas clustering is applied to multivariate clustering.
Meta-clustering which involves matching clusters across samples can also be considered to be a form of normalisation/clustering.

The question really boils down to whether we should normalise the samples or cluster each sample separately when presented with a choice.

In genotyping, we have a large number of markers and too few replicates to do within sample clustering, so normalisation is usually applied before data pooling.
Clustering is then applied on the pooled data.
In genotyping, normalisation is faciliated by having the same number of markers per sample.
Features are directly comparable.
On the other in hand in flow cytometry, there is much variation in the count of cells per sample.

In flow cytometry, we have much large number of cell than markers per sample,
so clustering tends to happen within a single sample in order to identify cell populations.
Normalisation can be done after the clustering to match and align the MFIs across samples.
However, while this transform is appropriate for bead data which is known to be identical,
it is not usually appropriate for biological data since it removes between sample biological variation.
Normalisation can also be done before the clustering, to aid the clustering (fixed gating),
to allow for pooling of samples, which can be useful in order to identify rarer cell populations,
to improve the signal-to-noise-ratio or to define a transform which can be used to align the data.
In my thesis, I have used both normalising before and after clustering.
In \Cref{chapter:il2} and \Cref{chapter:kir}, I have have attempted to normalise

\paragraph{Normalising after clustering: good for matching cluster across samples}

In the case when the centre of mean of the gate has moved, after a few E iterations of the EM algorithm for example, it possible to realign the gates on the data.
The advantage of this approach is that it doesn’t require all the data to be moved.
But normalisation is still required in the meta-clustering step to compare properties of the clusters.
Normalisation is effectively analogous to meta-clustering since we are attempting to match cell populations across samples.  
Normalisation facilitates matching clusters across datasets but in doing so can remove meaningful biological variation.  

The samples are related and we wish to exploit this structure without forcing data to be perfectly comparable.
A less biased approach is to allow for different cluster location and shapes across datasets and instead incorporate the clustering in some sort of hierarchical framework.  
Normalisation or meta-clustering is a necessary step when data is pooled across studies.


\paragraph{Normalising before clustering: good for pooling to find identify rare populations}

If applied before the clustering it enables data pooling to identify clusters across all samples.
Pooling may improve the coefficient of variation of populations but also facilitates
matching clusters across samples, a step known as meta-clustering.
Pooling is crucial for identifying rare groups which are difficult to identify within one sample.
If the alignment isn’t perfect, the clustering results can be refined across all samples.

Theoretically, if flow cytometry data was perfectly aligned across days then gates should not be moved, unless they are dependent on some internal control.  
For example a threshold gate on a positive subset may be defined in relation to
another population of cells.

As discussed in \Cref{chapter:il2ra},
the cell phenotypes usually measure, the mean intensity on a particular marker, or the percentage of cells can be more or less sensitive to the position of the gate,
depending on the shape of the distribution.


\paragraph{Conclusion}

Whilst normalisation is generally a necessary process for pooling data or comparing across samples, over correction can be detrimental to the analysis.
Normalisation is meant to be a simpler preliminary step to simply the clustering,
however, in flow cytometry, because of batch effects and unequal number of events per sample, normalisation can be as challenging as performing the gating.
%generally relies makes big assumptions and leads to major changes in the data.

However, when they are insufficient data points, pooling is necessary.
Clustering sometimes relies on pooled samples when they are not sufficient data points
to form clusters such as in the case of rare subsets like 3-0 in KIR or Tregs in flow cytometry data.
In this situation some form of normalisation is usually necessary.
Unfortunately, multivariate normalisation is not always trivial and relies on defining linear transforms. 



\paragraph{Some issues with normalisation}

Normalisation removes unwanted experimental variation to make data comparable even when collected on different days,
processed with different protocols or analysed with different instruments.
However distinguishing between what is unwanted and what is meaningful variation relies ultimately on a bias-prone judgement call.

As we have seen, different normalisation approaches make different assumptions about what is unwanted variation and the shape of the data.
The actual choice of normalisation depends on the characteristics of the data we wish to compare.

The Bayesian question of relative weight of prior vs data is very relevant to gating.
The position of the gates are the priors. Is it better to have an absolute definition/threshold or should the gates be relative to the data?
 
One issue with normalisation based on peak alignment is that the noise variation is greater than the biological variation between samples, so much so that in certain samples, the peaks are only identifiable after preliminary gating of lymphocytes.

We have only considered the univariate density function here but identification can be extended to the multivariate case.

\paragraph{Normalisation of multivariate distributions}

%The purpose of principal component correction is to remove unwanted correlation which may exist between samples or GC content.  
When applying a linear transform to multivariate data the correlation between the dimensions is preserved since the multiplicative factors cancel out.
However the covariance changes.
Applying a non-linear transform also changes the correlation.
If the data is binned using flowBin or clustered using SPADE then a transform could be a mapping like Earth Moving to make both distributions of event proportions identifical across samples.

\paragraph{Combining transforms with clustering}

Choosing an optimal transform in flow data is not trivial.
Transforms tend to be channel specific and sometimes even sometimes sample specific.
%Per sample transform required like flowClust but using the sliding window approach.
%Some prior knowledge is required to know what makes a sensible density plot.
%Care needs to be taken not to introduce spurious peaks.
The FlowClust solution proposes estimating the Box-Cox parameter as part of the clustering using a numeric optimiser.
However the parameters need to be transformed back, for allowing for a different transform per sample.
This is an issue if comparing intensity data but obviously if comparing cell ratios then the transform is of little consequence.
Nonetheless the transform can have an important effect on the clustering result.

Clustering is an iterative method of discovery not a fully automated process.
%Some tuning is required
It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.  
Clustering can also be viewed as a latent variable problem where the cluster labels are considered to be the missing data.



\section{ Method of determining phenotype affects power to detect genetic association }

%Decomposing the variance into tech

Consistency is an important factor in identifying clusters.
Decomposing the variance into within sample and between variation

Subset analysis is also very important because of a phenomenon known as Simpson’s paradox whereby a trend that is visible in a group may not hold in all subsets of the group and may even be reversed in certain subsets of that group.

When a hierarchical gating approach is taken, errors at the top of the hierarchy propagate down the tree.
%This approach is chosen for manual gating as it agrees with our very linear way of thinking.
This complicates an automated approach to step by step gating since errors which occur in earlier steps of the gating may not be recovered from.

In the same way wrongly gating samples can lead to outliers which can create false positive association due to their high leverage,
fixed gating can lead to all samples have the same profile.

\subsection{Reproducibility}

Reproducibility in flow cytometry is challenging.
While in genetic data, the number of probes across arrays is constant and identifiable,
flow cytometry data can contain very different number of events between samples.
Also distinguishing staining noise from actual differences in cells biology requires a certain level of prior knowledge which is difficult to implement programmatically.
Subsequently, a sample from the same individual analysed on different days can have a very different profile.
Biologists tend to have strong opinions on which markers are stable and objectively it does appear that certain stains are much more stable than others.
As seen in \Cref{chapter:il2ra}, this within-individual variation greatly compromises statistical power in detecting between-individual effects.

In long-running experiments when samples are analysed over a period of months, some noise may be attributed to variation in instrument sensitivity.
In \Cref{chapter:il2ra}, we showed we can account for this using beads.
However beads do not capture variation on shorter time scale or due to batch effects.
Staining and in particular intra-cellular staining, of internal markers such as STAT5 and FOXP3, is subject to batch and day variation.

Another approach is to align the peaks of the univariate distribution in the whole sample as was done for qPCR $\Delta$Ct values in \Cref{chapter:kir}.
However, in the flow cytometry samples we studied, we found that the peaks cannot always be identified reliably and choosing the right window-size parameter for peak finding algorithms is channel specific and not trivial.
Also mismatching of peaks in alignment is more detrimental to repeatability than not any doing normalisation.

%Therefore clustering is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and error.  
%It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.  
%Clustering can also be viewed as a latent variable problem where the cluster labels are considered to be the missing data.




%Lessons learned
%Things I have learned: philosophical discourse

%\section{ future application of methods }
\section{ The future }

The methods discussed in this thesis suggests that flow cytometry analysis can be automated.
There are however a number of outstanding challenges, some technical, some more philosophical, in applying these methods.

\paragraph{Seeing is believing}

Statisticians, biologists, we all like to visualise our data.
We rely on visual inspection as a quality control check, learning about properties of the data like for example if the data
is skewed or symmetrically distributed, looking for patterns, confirming results, spotting outliers.
While visualisation works well for up to three dimensional data,
information is lost when higher dimensional datasets are decomposed into a series of two-dimensional projections.
Clusters which exist in higher dimensions do not necessarily map to clusters in two dimensions.
%Clusters are not identifiable in two dimensions, in fact for clusters which are thus ignoring a large number of points.
%In fact K points are only garanteed to be separable in at least K+1 dimensions.
%An upperbound on the number of clusters in the data is the product of the univariate modes,
%whereas a lower bound is the maximum number of modes in one dimension.
This has motivated research into how to visualise high-dimensional data in two-dimensions with minimal loss of information.
In \cref{chapter:il2}, I presented the SPADE which relies on a network visualisation of a dataset using a minimum spanning tree.
While I agree that visualising high-dimensional data using network representations can be insightful,
beyond a certain number of nodes, network visualisations quickly become uninformative.
Certainly in the case of flow cytometry some clustering or data smoothing is required to reduce the number of points.  
%There are many more which take more probabilistic approaches like stochastic neighbour embedding
%My opinion is that this fascination for visualisation boils down to a distrust of computer vision
%visualisation can also be misleading and lead to oversimplifications

%An important part of our job is to unearth trends or make them visually obvious.


\paragraph{Experiment vs analysis: a communication problem}

Analysing flow cytometry data has brought to light the many issues surrounding the generation and analysis of data.
Perhaps the greatest obstacle to standardising analysis is our inherent dislike of standards.
However, simple things like naming conventions waste precious man hours.
To err is human,  which is why automation is necessary.
%At least automation is consistently and predictably wrong.

On the other hand, statisticians need to have an understanding of: the underlying technology, the purpose of the experiment.
For example, in flow cytometry, a large number of events are debris of no biological interest.
Similarly some patterns may be simply staining artefacts or from sample prepartion (permeabilisation).

A great deal of time was also spent on seeing if clustering could be fully automated. 
One approach is to select large K and then merge clusters together with flowMerge.
My approach was generally to select a K that gave consistent clustering results across samples.

While I am sure these observations are not only specific to flow cytometry, they are more apparent perhaps in flow given how much freedom flow cytometry offers both in terms of generation and analysis.


\paragraph{ Incomplete experiments and small sample sizes}

The majority of experiments undertaken in flow cytometry are pilot experiments or tubes ran to test and optimise panels.
Pilot experiments
%may be motivated by a moment of inspiration and leads to the generation of flow data is often an adhoc thought process,
are often implemented with varying degrees of thoroughness.
Normalisation beads or controls are not consistently which complicates analysis.
While the FCS files are saved and analysed by the person who generated the data, the naming and documentation is incomplete
making it hard to automate the analysis of these data.
Since the FCS file does not contain sufficient metadata to understand the context of the experiment, the name of the FCS
file is typically used to map the sample back to the donor in order to retrieve covariates such as disease status, age, sex or genotype.
A lot of my work unfortunately has involved dealing with typos and inconsistencies in these file names.
Also channel names as given in the FCS files are not always consistent across experiments.
When the FCS file do contain metadata it does not always match the naming of the file.
For example the data as strored inside the FCS file did not match the date given in the filename.
In particular, FCS files do not contain sample identification information, which makes matching back to genotypes cumbersome and error-prone.
%Some minor issues for example, when experiments are done over night the dates might mismatch

Typically flow cytometry experiments do not have large number of samples because in most labs, sample preparation and running tubes on the flow
cytometer are manual operations.
%Futhermore the fluorochrome antibody mix used per tube costs in the order of \textsterling20
%the method of operating flow cytometer is still quite manual in most labs.
However there are labs where these processes have been automated with robotics
%to feed flow tubes
and consequently may run thousands of samples a day.
Automatic methods are more pervasive in those labs since manual analysis is no longer a viable option.
%Within our lab, samples from longitudinal experiments are more common and come in over a long period of time and need to be analysed on the day or frozen.
Presently most flow experiments contain too few samples to do well-powered association testing.


\paragraph{ Stability of markers }

Staining in flow cytometry is notoriously noisy.
Even when using the same fluorochrome-antibody panel and the same PMT voltage, the shape and location of clusters is not stable.
The treatment of the cells can also lead very different scatter patterns (example Tony vs Marcin).
While the scatter channels are very noisy because of debris, for a given panel and experimental protocol,
the location of the clusters should not move much on the scatter channels because the morphological attributes of the cells should not be dependent on staining titration.


\paragraph{ Flowjo interface with R}

FlowJo is the main tool used by immunologists for identifying groups of cells in flow cytometry.
The unit of work in FlowJo is the workspace in which FCS files are first loaded and then gated.
The workspace also saves the cell populations statistics which need to be updated when the gates move.

Unfortunately, parsing FlowJo workspaces in order to extract manual gates is not straighforward.
Although there are several BioConductor packages designed to import and parse flowJo workspaces, flowUtils gatingML, flowJo, flowWorkspace,
I have found the R/FlowJo interface to not be very reliable, although the flowWorkspace was able to parse it returned the statistics calculated in FlowJo were wrong.
This is probably why Vincent Plagnol developed his own XML parser to extract gates from flowWorkspace files but this approach is time-consuming as it requires in knowledge of the FlowJo XML schema which frequently changes.
Furthermore the gate coordinates in FlowJo are often imprecise.
In fact I found that just loading an FCS file into FlowJo and reexporting it changes the data!
In the end I found the best solution was to export CLR files which are simply the classification results from FlowJo.
Unfortunately, instead exporting these files in a compressed memory-efficient format binary format, FlowJo, in its unfathomable wisdom, exports them in text which results in very large files and makes exporting of all clustering results impractical.
Hence I have often resorted to exporting only a few CLR files from which I can estimate the gate coordinates.
One thing however that lacks from the CLR format is to retrieve the gate coordinates is the dimensions in which the gate is defined.
One method of approximating gate coordinate is to calculate the mean and covariance of a CLR cluster and to use the Mahalanobis distance,
hence approximating the cluster with an ellipse.
However, it helps to know in which dimensions the gate was defined manually.

Another solution to including manual gates in R without relying on their coordinates in FlowJo
is to draw polygons in R and use the \Rfunction{in.polygon} to extract points in the polygon.
%\paragraph{Inconsistencies in file naming}

\paragraph{Mass cytometry}

Time of flight cytometry (CyTOF) is a biotechnology which combines mass spectometry with cytometry.
The throughput is not as high as fluorescence flow cytometry but the number of markers which can measure up to 34 markers.
Also no side and forward scatter information is given.
It cannot be used for sorting as the cells are destroyed when measured.
Since it does not report side and forward scatter live/dead markers tend to be used instead to spot debris.

\section{Summary}

Large datasets have allowed us to see much finer biological variation both in genotypes and cell subsets.

Sometimes taking a different view of the same dataset, by doing a different type of experiment like qPCR or adding markers as in flow cytometry,
can help us uncover patterns in the larger datasets which we did not know existed.
For example, qPCR allowed us to discover SNPs predictive of KIR3DL1/3DS1 copy number.
Considering pSTAT5 response at different doses allowed us to discover a responsive subset of cells we previously ignored.

Although large datasets can support methods with larger number of parameters as they are less prone to overfitting,
it is easy to fall into the trap of applying over complex methods to account for all the intricacies of the data,
when in practice, simpler methods may perform as well and are much faster and easy to implement.
Simple methods can also be combined to reach a consensus and this popular machine learning approach known as boosting
may increase performance at the expense of interpretability.

Interpretability is perhaps one of the more important issues in order to encourage biologists to use these methods.
While mixture modelling approaches are conceptually close to manual gating, probabilistic populations are not intuitive to biologists,
so their true power cannot be fully exploited.
Biologists enjoy manual gating becomes it gives them the freedom to draw somewhat arbitrary exclusive gates.
This freedom however comes at the cost of exacerbating the disagreement in standards and definitions in immunology.
While discrepancies in gate positions are unlikely to make much of an impact on the MFI and relative proportion of common cell populations,
they can make a big difference on rarer cell populations such as regulatory cells.

In order to encourage the use of these methods, biologist need to be lured in by incrementally habituating them to these tools.
Completely removing the two dimensional visualisation for example might be discouraging.
Also the cellular hierarchical view is so deeply engrained in the minds of certain biologists that presenting them the clustering results
in a different order would perturb them.  Although attractive, this hierarchical model misses populations and imposes a directionality
in cell lineages which may not always be correct.  In the same way that phylogenies change as new animal species are discovered,
the cell lineage model should be influenced by the discovery of new cell subsets.

A first step is to use the manual gates but to allow them to move with the data.
%This can occasionally be achieved by centering the ellipse on the mean of the datapoints which fall within the gate.

While in my opinion automated clustering need to be applied more widely in flow cytometry data analysis,
hence it is important that we continue developing these methods because biologists will need to resort to fully automated method once the number of parameters
or number of samples becomes unmanageable.
However, I recognise that these methods require some level of expertise and decent visualisation to guide (and reassure?) the user.
Although, over-reliance on visualisation can mislead the analysis of high-dimensional because clustering is always projected back to two dimensions
or linear combinations of dimensions.
The automatic gating of flow cytometry community is strong with a lot of contibutions to BioConductor and the GenePattern web interface from the Broad Institute.
In particular two labs,
Raphael Gottardo at the Fred Hutchinson Cancer Research Center in the USA
and
Ryan Brinkman at the Terry Fox Laboratory in Canada,
are central in developing auto gating software and bring together,

In Stanford, Gary Nolan lab are using mass cytometry to analyse cell heterogeneity in cancer.
These high-dimensional datasets require visualisation and Dana Pe'er group at Columbia University have devised various tools to do so.

I have also identified regions of flow cytometry which need more work such as normalisation and selection an optimal transform.
Both of these can be included as part of the clustering step.
Logarithmic transform greatly influence the identification of lower intensity populations which overlap into the negative range.
The wrong transform can introduce splits giving rise to spurious cell populations.
This is why applying a straightforward arcinsh transform like is done in spade is wrong.
In FlowJo, the transform is selected visually, given the knowledge of what cell populations to expect.
The only existing automated  methods of optimally selecting a transform that I am aware of are flowTrans and flowClust.
FlowTrans assumes an underlying Gaussian distribution and used maximum likelihood to estimate the optimal transform parameter.
But there is still the question of which transform function to apply?
FlowClust applies a Box-Cox transform for which the lambda parameter is estimated as part of the ML estimation.

There is also a growing need for non-proprietary software which integrates well with the manual analysis.
The openCyto BioConductor package currently being developed by Gottardo
The automated methods need to complement the manual methods for now, so that the change from manual to automated happens gradually.
On the other hand by continuously benchmarking automated analysis against manual,
we are not exploiting the true power of automated algorithms which is to teach us new biology or to put back into question our hiearchical view of immunology.
As an example I ran flowClust unsupervised with a large number of clusters and then picked the cluster which gave the best association with each SNP.
This is the idea which was explored with flowMeans.  The issue however is the metaclustering step of matching clusters across samples is not trivial especially if
there is a lot of noise between samples.


% I admit there has been a lack of simulations but flow data is hard to simulate.
% simulate small responsive population?

No matter how sophisticated the method, there is no replacement for good data.
Being able to make this judgment call between good and bad data necessitates understanding of the experimental context.
This prior knowledge is acquired through having seen a large number of samples and hence not encoded into an automated method.

Processing in larger batches or perhaps reducing the human element in flow cytometry is a first step towards automation.
%http://www.aber.ac.uk/en/cs/research/cb/projects/robotscientist/
However as the number of samples grows so will the need for computational methods
and the gap between the biological of computational way of thinking will become less striking.







