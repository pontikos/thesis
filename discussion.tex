\chapter{Discussion}

%\epigraph{ {\fontfamily{frc}\selectfont \foreignlanguage{french} { C'est le temps que tu as perdu pour ta rose qui rend ta rose importante. }} }
%Tu te jugeras donc toi-même, lui répondit le roi. C'est le plus difficile. Il est bien plus difficile de se juger soi-même que de juger autrui. Si tu réussis à bien te juger, c'est que tu es un véritable sage.
%\epigraph{ {\fontfamily{frc}\selectfont \foreignlanguage{french} { Il n'y a pas de citadelle indefendable, il n'y a que des citadelles mal defendues. }} }
\epigraph{
{\fontfamily{frc}\selectfont \foreignlanguage{french} {
L'avenir n'est jamais que du présent à mettre en ordre, tu n'as pas à le prévoir, mais à le rendre possible.
%Pour ce qui est de l'avenir, il ne s'agit pas de le prévoir mais de le rendre possible.
}}}


\section{Value of clustering in flow cytometry}

Throughout my thesis I have considered methods of computationally analysing flow cytometry and genetic datasets in order to characterise the immune cell types and the gene variants which could influence \gls{T1D} risk.
These methods promise to be more efficient, formal, consistent, objective and better at dealing with uncertainty than manual analysis.
%In the case of flow cytometry data, automating gating can also provide the added benefit of formalising manual gating, which remains to this day a very subjective, ad-hoc, process.
If these automatic methods can deliver this should lead to improved reproducibility and more powerful association testing.

Ultimately the goal of normalisation and clustering is to allow for statistical association testing of genetics with biological traits.
%Decomposing the variance into tech
This is challenging because most genetically inherited biological traits are complex, that is, a significant proportion of the variation is explained by the cumulation of a large number of small genetic effects.
%a multitude of smalls effects which cumulatively explain
%Most effects in biological variation are small, they therefore require large sample sizes and

The most studied factor which influences the statistical power to detect such effects is sample size, and we are witnessing ever-increasing sample sizes thanks to dropping genotyping and sequencing costs.
%This is because the standard deviation tends to shrink with the inverse of the sample size.
However, there is still much work to be done on phenotype reproducibility.
Poor reproducibility is in part due to the heterogeneity of traits which might be classified as having the same phenotype.
Disease phenotypes are known to differ depending on the individual and the stage of the disease.
%If the disease is diagnosed earlier then the associated phenotype may be recurrent auto-antibody positivity or if diagnosed late, high blood glucose.
In most diseases, case heterogeneity is a major confounder and one which needs addressing by collection of additional clinical covariates during recruitment.

Since these disease phenotypes are perhaps too coarse,
%as I have touched upon in \Cref{chapter:intro},
this has motivated looking at better defined quantitative intermediate phenotypes, also known as endophenotypes.
It is expected that these should have stronger correlation with genotype since they are closer to gene expression.
Endophenotypes which I have studied in my thesis are cell phenotypes such as, CD25 protein expression on T cell subsets (\Cref{chapter:il2ra}), pSTAT5 \textit{ex vivo} response in \glspl{PBMC} (\Cref{chapter:il2}) or the relative cell subset frequency over parent cell population.
%or expression of surface markers 
%For flow cytometry, genetic ananlysis just needs small samples containing the number of samples that can be collected

%These endophenotypes are more quantitative, especially with high-throughput technologies such as flow cytometry, in practice there are still factors which influence the accuracy at which these can be measured.
The methods by which these endophenotypes are quantified now becomes extremely relevant.
I will especially focus on flow cytometry analysis since these were the focus of my thesis and where they are the most challenging to apply.

%For example in \Cref{chapter:il2ra}, I illustrated how minor changes in the clustering can influence the relative frequency and MFIs of the cell populations under study and the strong effect of outliers on the results.
%Some other influential factors were, staining batch effects as seen in \Cref{chapter:il2}, or long-term instrument sensitivity as seen in \Cref{chapter:il2ra}.



%\section{ Pre-requisites }
\section{ Challenges in automation of flow gating }

The methods discussed in this thesis suggests that there are many scenarios in which flow cytometry analysis can be automated.
There are however a number of outstanding challenges, some technical, some methodological and even philosophical, in applying these methods.

%However, assessing the performance of computational methods is a matter of debate and very much data-dependent.
%Nonetheless, I will attempt here to give an overview of important practical and theoretical considerations when implementing and applying these methods, in the context of my thesis.

\subsection{Performance: compute time and memory usage}
While the applications that have been studied in this thesis don’t require real-time analysis (unless we were working on cell-sorting), there are still some practical limitations on the amount of memory and compute time demands.
It is well in computer science that these resources are interchangeable.
For example for clustering, methods which rely on global knowledge of the data, such as computation of the complete pairwise distance matrix, have a large initial memory footprint ( data matrix of size $\frac{N \times (N-1)}{2}$ ), but only require one pass of the data.
A solution to the clustering solution is reached within just a few computational steps, by for instance,
selecting a distance cut threshold on the dendrogram, built from hierarchical clustering, in order to define K clusters. 
On the other hand, methods like K-means necessitate a much smaller memory footprint since they only compute the distance of every point to the K cluster means ( data matrix of size $N \times K$, where usually $K << N$), but on the other hand, several updates of the matrix are required until the cluster centers are fixed.
The choice of which method to apply is very much data dependent.
For ungated flow cytometry, data matrices  contain over a million rows, so the complete pairwise distance matrix is too large to fit in memory.
I found that for the default R implementation, the maximum allocatable vector size is 1672.4 Gb, hence the distance matrix computation was only feasible in subsets of the order of $10000$, like for example the CD4\positive lymphocyte subset, or data downsampled using the SPADE or RPART approaches in \Cref{chapter:il2}.
The data downsampling as performed by SPADE, relies on first estimating the local density at each point and then preferentially thinning the data in regions of high density to even out the density across the whole sample.
The local density estimation step is itself computationally intensive as it needs to consider the distance from a single point to all other points in the sample to find the number of points lying within a certain radius.
I found this step could be greatly sped up with \gls{ANN} which uses the K dimensional trees (KD tree) lookup method.
KD-trees are a space-partitioning data structure for organising points in a k-dimensional space which makes for an efficient
way of storing a high-dimensional dataset to lookup proximal datapoints.
This approach could also in theory be used to reduce the number of datapoints considered when applying mixture models \citep{McLachlan:2004uw}.

\subsection{Consistency}
Consistency can be defined as the variation in a method's output in relation to its input.
If a clustering algorithm is consistent, one would expect that small perturbations to the data should lead to small changes in the clustering outcome.
However, algorithms which rely on initialisation using random starting positions like K-means can reach different clustering solutions, even on the same data.
To guard against this when running K-means, I usually specify the initial clusters or run it with a large number of multiple restarts in order to increase the chances of finding a globally optimal solution.
As I will discuss later, a relatively small number of outlier events which occur commonly within a flow cytometry experiment, due to debris or cells clumping together, can also be detrimental to consistency and automated gating algorithms need to be robust to these.

\subsection{Accuracy}
When labelled data is available, the accuracy of a method is defined based on how frequently a method assigns the correct label.
Estimating accuracy relies on the existence of a test dataset, typically labelled using manual analysis or some other method.
In \Cref{chapter:il2ra}, accuracy was assessed by comparing the cluster proportions and means with those obtained from manual gating.
%In \Cref{chapter:il2}, the accuracy was determined in terms of RSS of the pSTAT5 response.
In \Cref{chapter:kir}, I used qPCR labelled data to assess the prediction accuracy of the \gls{knn} classifier.
However, labelled data may not always be available especially in the case of flow cytometry data.
Also, in the context of flow cytometry, even when labelled data is available, this approach may not always be ideal,
as it is merely comparing the relative agreement between methods rather than the objective truth.
A more useful alternative may be is instead to assess the prediction accuracy with clinical outcome, case-control status or genotype.
This is usually implicitly measured when doing an association test.
%The consistency-accuracy tradeoff is analogous to the variance-bias trade-off in statistics.
%Consistency and accuracy are typically measured on simulated data.

\subsection{Interpretability}
While a method might be accurate and consistent, it may be difficult to interpret the results, much like a black box.
%In biology, we are often more interested in the process behind the result than in the result itself.
Improving the accuracy of a model, by adding parameters can obfuscate the relationship between the input parameters and the clustering output.
\Acrfull{RF} and neural networks are examples of methods from which it is difficult to extract an interpretable model to justify the result.
This is an issue because
%generally people (and especially inquisitive minds like scientists) are not comfortable with applying methods they don’t understand.
as part of the iterative process of knowledge discovery, it is important to understand what combination of features make objects distinguishable, especially in biology.
%Since fully automated methods are hard to assess.
%Sometimes additional information which is available from previous studies can be used to guide the clustering.


%\paragraph{Parallelisation}
%Provided the data can be split into independent chunks, then this feature can be exploited with parallelisation.
%By make astute use of parallelisation, one can marry local and global methods to make efficient use of the computational facilities
%available such as compute farms of many nodes with limited memory and processing power.
%In such a scenario, each compute node can work on subset of the data to compute intermediate results which can then be combined.
%In the context of flow cytometry however, samples were not split but analysed individually on each compute node.
%In the future, it may be possible to exploit the hierarchical nature of the gating to conduct parallel analysis of independent subsets.

%\section{Clustering with prior knowledge}
%\section{Challenges of computational methods}
%\section{Dealing with noise using prior knowledge}

%Having gone through some of the theoretical and practical properties of computational clustering methods, I will now address challenges which are more pertinent to the data on which these methods are applied.
%%One compromise is that by adding more markers to delineate cell populations we are increasing the number of potential clusters and thus the tail of distribution of cluster sizes
%Variation due to batch effects in the preparation or sample ascertainment,
%or insufficient number of flow cytometric parameters


%Futhermore another concern with clustering and statistics in general is distinguising rare subsets from outliers.

%Individual outlying datapoints are harder to spot in smaller sample sizes and we are also generally more reluctant to exclude samples.
%In \Cref{chapter:kir}, a whole qPCR plate was excluded because the $\Delta$Ct distribution didn’t align well even after normalisation.
%Later I found that qPCR samples could have also been scored by considering their background Ct median and spread.
%This score would have been used to downweight these samples in the mixture model clustering which was used to assign them to copy number groups.  
%However in the case of systematic error, dropping individual samples could introduce bias.

%Another good means of spotting outliers, is visualisation by for example plotting summary statistics from multiple samples.
%Boxplots are the typical visualisation tool for viewing univariate intensity data but in flow cytometry where we typically have multivariate data, \gls{SPICE} presents an overview using pie charts of multiple samples \citep{Roederer:2011hy}.
%%An alternative visual representation are radarplots

%These types of scores help to capture the uncertainty in calling outliers.
%While certain outliers can be confidently called independently because their values lie at the extremes of the range of the instrument, others are less clear and should not be discarded completely because in light of new data they may be assigned to existing clusters or to new clusters.



%\section{Advantages and disadvantages of top-down clustering}
%
%Sometimes prior knowledge can be used to build a top-down tree-like approach to clustering, such as manual gating in flow cytometry as described in \Cref{chapter:il2ra} and \Cref{chapter:il2}.
%This approach is advantageous from a practical perspective, it is efficient because the dataset is reduced as points are filtered at each step, and as not all dimensions are considered at the same time, simpler univariate and bivariate clustering algorithms which use density can be applied.
%%Also only considering a subset of markers at a time is easier to interpret.
%It also conforms to the established hierarchical model of cell lineages which is deeply engrained in the minds of certain immunologists.
%%Methods which use a binary structure are easy to interpret as they conform to the manual gating strategy which provides rules on how to obtain the leaf subsets.
%%Reduces ambiguity.
%On the other hand, a disadvantage is that this approach can easily miss important cell populations.
%Furthermore, the construction of trees tends to very sensitive to small perturbations in the underlying data and errors propagate at each step.
%These inconsistencies become more apparent the deeper the hierarchy.
%Errors could be mitigated by introducing conditional probabilities at each step but this would come at the cost of conserving all datapoints.
%Another shortcoming of this top-down approach is that it imposes a directionality on cell subsets which may not accurately reflect the biology.
%It also imposes an ordering on the influence of the markers on the gating which may not be the most appropriate for every dataset.
%%It may be more ambiguous markers are gated on first.
%Although this hierarchical approach does have advantages, I would advocate also trying more unsupervised approach when sufficient data is available, for example \gls{RF}, to get some idea of the relative importance of the markers.
%%in the same way that phylogenies change as new animal species are discovered, cell lineages should also be influenced by the discovery of new cell subsets.
%%However, if no restriction are applied the rules can become over-complicated.
%%When a hierarchical gating approach is taken, errors in earlier steps propagate.
%%This approach is chosen for manual gating as it agrees with our very linear way of thinking.
%%This complicates automated approaches since errors which occur in earlier steps of the gating are not recoverable from.
%

%\section{ Distinguishing between outliers and rare subsets }
%\section{Prioritising normalisation or clustering}
%\section{To move data or to move gate: normalisation vs clustering}

%Sometimes it is better not to normalise and normalisation may hide the true biological effect.
%The question really boils down to whether we should normalise the samples or cluster each sample separately.
%Normalisation is effectively analogous to meta-clustering since we are attempting to match cell populations across samples.  
%Normalisation facilitates matching clusters across datasets but in doing can remove meaningful biological variation.  
%Theoretically if the data was perfectly aligned across days then gates should not be moved.  
%Threshold gates should not move unless they are relative to some internal control.
%The cell phenotypes which are usually measured are the mean intensity of a particular channel, or the percentage of cells.
%Depending on the shape of the distribution these can be more or less sensitive to the position of the gate.
%Normalisation removes unwanted experimental variation to make data comparable even when collected on different days,
%proccessed with different protocols or analysed with different instruments.
%However distinguishing between what is unwanted and what is meaningful variation relies ultimately on a bias-prone judgement call.
%As we have seen, different normalisation approaches make different assumptions about what is unwanted variation and the shape of the data.
%The actual choice of normalisation depends on the characteristics of the data we wish to compare.
%We have only considered the univariate density function here but identification can be extended to the multivariate case.
%Next let's consider normalisation of multivariate distributions.  
%The purpose of principal component correction is to remove unwanted correlation between samples.  
%When applying a linear transform to multivariate data the correlation between the dimensions is preserved since the multiplicative factors cancel out.
%However the covariance changes.
%Care must be taken when applying a non-linear transform.  
%The samples are related and we wish to exploit this structure without forcing data to be perfectly comparable.
%A less biased approach is to allow for different cluster location and shapes across datasets and instead incorporate the clustering in some sort of hierarchical framework.  
%Normalisation or meta-clustering is a necessary step when data is pooled across studies.  
%One issue with normalisation based on peak alignment is that the level of there is more noise variation than biological variation between samples so that in certain samples,
%so much so that in certain samples the peaks are only identifiable after preliminary gating of lymphocytes.  

%\paragraph{Normalising after clustering: good for matching cluster across samples}
%
%In the case when the centre of mean of the gate has moved, after a few E iterations of the EM algorithm for example, it possible to realign the gates on the data.
%The advantage of this approach is that it doesn’t require all the data to be moved.
%But normalisation is still required in the meta-clustering step to compare properties of the clusters.
%Normalisation is effectively analogous to meta-clustering since we are attempting to match cell populations across samples.  
%Normalisation facilitates matching clusters across datasets but in doing so can remove meaningful biological variation.  
%
%The samples are related and we wish to exploit this structure without forcing data to be perfectly comparable.
%A less biased approach is to allow for different cluster location and shapes across datasets and instead incorporate the clustering in some sort of hierarchical framework.  
%Normalisation or meta-clustering is a necessary step when data is pooled across studies.
%
%\paragraph{Normalising before clustering: good for pooling to find identify rare populations}
%
%If applied before the clustering it enables data pooling to identify clusters across all samples.
%Pooling may improve the coefficient of variation of populations but also facilitates
%matching clusters across samples, a step known as meta-clustering.
%Pooling is crucial for identifying rare groups which are difficult to identify within one sample.
%If the alignment isn’t perfect, the clustering results can be refined across all samples.
%
%Theoretically, if flow cytometry data was perfectly aligned across days then gates should not be moved, unless they are dependent on some internal control.  
%For example a threshold gate on a positive subset may be defined in relation to another population of cells.
%
%As discussed in \Cref{chapter:il2ra}, the cell phenotypes usually measure, the mean intensity on a particular marker, or the percentage of cells can be more or less sensitive to the position of the gate, depending on the shape of the distribution.

%\paragraph{Some issues with normalisation}
%
%Normalisation removes unwanted experimental variation to make data comparable even when collected on different days, processed with different protocols or analysed with different instruments.
%However distinguishing between what is unwanted and what is meaningful variation relies ultimately on a bias-prone judgement call.
%
%As we have seen, different normalisation approaches make different assumptions about what is unwanted variation and the shape of the data.
%
%The choice normalisation depends on the characteristics of the data we wish to compare.
%
%The Bayesian question of relative weight of prior vs data is very relevant to gating.
%%The initial position of the gates are the priors.
%In \Cref{chapter:il2ra} the question of whether it better to have an absolute threshold or one relative to the data, showed that keeping the same threshold for all samples analysed on the same day lead to better reproducibility than the per sample user-set threshold.
 %
%One issue with normalisation based on peak alignment is that the noise variation is greater than the biological variation between samples, so much so that in certain samples, the peaks are only identifiable after preliminary gating of lymphocytes.
%
%We have only considered the univariate density function here but identification can be extended to the multivariate case.
%
%\paragraph{Normalisation of multivariate distributions}
%
%%The purpose of principal component correction is to remove unwanted correlation which may exist between samples or GC content.  
%When applying a linear transform to multivariate data the correlation between the dimensions is preserved since the multiplicative factors cancel out.
%However the covariance changes.
%Applying a non-linear transform also changes the correlation.
%If the data is binned using flowBin or clustered using SPADE then a transform could be a mapping like Earth Moving Distance to make both distributions of event proportions identifical across samples.
%
%\paragraph{Combining transforms with clustering}
%
%Choosing an optimal transform in flow data is not trivial.
%Transforms tend to be channel specific and sometimes even sometimes sample specific.
%%Per sample transform required like flowClust but using the sliding window approach.
%%Some prior knowledge is required to know what makes a sensible density plot.
%%Care needs to be taken not to introduce spurious peaks.
%The \Rpackage{flowClust} proposes estimating the Box-Cox power transform exponent as part of the clustering using a numeric optimiser.
%However the parameters need to be transformed back, for allowing for a different transform per sample.
%This is an issue if comparing intensity data but obviously if comparing cell ratios then the transform is of little consequence.
%Nonetheless the transform can have an important effect on the clustering result.
%
%Clustering is an iterative method of discovery not a fully automated process.
%%Some tuning is required
%It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.  
%Clustering can also be viewed as a latent variable problem where the cluster labels are considered to be the missing data.
%
%\paragraph{Conclusion}
%
%Whilst normalisation is generally a necessary process for pooling data or comparing across samples, over-correction can be detrimental to the analysis.
%Normalisation is meant to be a simpler preliminary step to simply the clustering,
%however, in flow cytometry, because of batch effects and unequal number of events per sample, normalisation can be as challenging as performing the gating.
%%generally relies makes big assumptions and leads to major changes in the data.
%
%When there are insufficient data points, pooling is necessary.
%Clustering sometimes relies on pooled samples when they are not sufficient data points
%to form clusters such as in the case of rare subsets like 3-0 in KIR (\Cref{chapter:kir}) or Tregs in flow cytometry data.
%In this situation some form of normalisation is usually necessary.
%Unfortunately, multivariate normalisation is not always trivial and relies on defining linear transforms. 


%\section{ future application of methods }

\subsection{Choice of transformation}

Flow cytometry and fluorescence data in general, tend to be highly positively skewed.
This is problematic for most clustering algorithms which assume constant variance across the data range.
%rely on on variance decomposition.
While the skewness can be reduced by the means of tranforms based on the logarithm function, care needs to be taken as these transforms influence the shape of lower intensity populations, especially those which overlap into the negative range as illustrated in \Cref{figure:logicle-transform-w}.
Certain flow cytometry packages like the \Rpackage{curvHDR} erroneously apply an arcinsh transform, however such a function introduces a split in the data density around zero, giving rise to spurious cell populations.
In FlowJo, the transform can be customised visually by the manual gater, given the knowledge of what cell populations to expect.
The only existing automated  methods of optimally selecting a transform that I am aware of, are the \texttt{R} packages \textsf{flowTrans} and \textsf{flowClust} \citep{flowTrans,flowClust}.
\textsf{flowTrans} assumes an underlying Gaussian distribution and uses \gls{ML} to estimate the optimal transform parameter.
\textsf{flowClust} applies a Box-Cox transform for which the lambda parameter, the exponent of the transform, is estimated as part of the \gls{ML} estimation.
Nonetheless as I discussed in \Cref{chapter:intro} and showed in \Cref{figure:logicle-transform-w}, it is not always clear what transform to apply and I would argue, aiming for a Gaussian distribution is a suboptimal criterion given the multimodality of the data.
When the number of populations is unknown, one approach could be to estimate the transformation parameter as part of the clustering process.


\subsection{Visualisation of higher dimensional datasets}

Visual inspection is a fundamental tool for quality control, discovery of data features like skewness or symmetry, looking for patterns, gene lists appearing in pathways, confirming clustering results, spotting outliers.
While visualisation works well for up to three dimensional data, information is lost when higher dimensional datasets are decomposed into a series of two-dimensional projections.
Clusters which exist in higher dimensions do not necessarily map to clusters in two dimensions.

Open repositories such as Cytobank\footnote{\url{https://www.cytobank.org/}} and FlowRepository\footnote{\url{http://flowrepository.org/}} \citep{Spidlen:2012hk} are encouraging researchers to share their annotated flow cytometry experiments along with their publications.
By combining data across experiments, both the number of samples and parameters measured are increasing.
Also an upcoming biotechnology, time of flight cytometry (CyTOF), which combines mass spectometry with cytometry will push the number of markers which can measured by experiment up to $34$ and potentially higher.
%, although the throughput is not as high as fluorescence flow cytometry.
%It cannot be used for sorting as the cells are destroyed when measured.
%Also, as it does not report side and forward scatter, live/dead markers are used instead to spot debris.
At the Gary Nolan lab in Stanford, and the CRI in the UK, mass cytometry is adopted to analyse cell heterogeneity in cancer.
The analysis of the type of datasets generated by this technology benefits greatly from the multidimensional visualisation techniques such as \gls{SPADE} \citep{Simonds:2011jh} and \gls{viSNE} \citep{Amir:2013jp}.
%These high-dimensional datasets require some form of low-dimensional visualisation and Dana Pe'er group at Columbia University has devised various tools to do so such as viSNE \citep{Amir:2013jp}.
%Clusters are not identifiable in two dimensions, in fact for clusters which are thus ignoring a large number of points.
%In fact K points are only garanteed to be separable in at least K+1 dimensions.
%An upperbound on the number of clusters in the data is the product of the univariate modes,
%whereas a lower bound is the maximum number of modes in one dimension.
%This has motivated research into how best to visualise high-dimensional data in two-dimensions with minimal loss of information.
In \Cref{chapter:il2}, I presented one these approaches, \gls{SPADE} which relies on a network visualisation of a dataset using a minimum spanning tree.
\gls{viSNE} is a more probabilistic approaches which uses stochastic neighbour embedding.
Visualising high-dimensional data using network representations is only informative if the number of datapoints is relatively small.
In flow cytometry, some clustering or binning of the data is applied to reduce the number of points.  
%There are many more which take more probabilistic approaches like stochastic neighbour embedding
%My opinion is that this fascination for visualisation boils down to a distrust of computer vision
%visualisation can also be misleading and lead to oversimplifications
%An important part of our job is to unearth trends or make them visually obvious.

\subsection{Ascertaining the number of clusters}


Certain transformations can facilitate the clustering task, however the challenge remains of determining the number K of clusters in a particular dataset.

In univariate data, a sliding window approach can be used to estimate the number of modes/peaks.
The number of peaks returned is influenced by the span of the sliding window.
A large window span will tend to oversmooth the data, leading to fewer peaks while with a smaller window more peaks will be called, but so will the chances of picking up spurious peaks.
The number of clusters is controlled by the window span parameter, and the exact relationship between this parameter and the number of clusters returned is data dependent.

%Over the course of my PhD, I have spent a great deal of time assessing whether clustering can be fully automated, i.e made truly non-parametric.
One approach to estimating K is to start with an upper bound for K and then merge clusters together with the \Rpackage{flowMerge}.
Another approach would be to select a K that gave consistent clustering results across samples.
However, finding a consistent K across clusters is not always possible because of sampling variation.
In particular, rare clusters may only be visible when a sample is sufficiently large, and hence not consistently identifiable across all samples.
%In cases when certain characteristics of the data are known, these can assist in the clustering especially with regards to identifying rarer subsets which 
In genetics for example, when genotyping low minor allele frequency variants, large samples sizes are required for homozygous individuals to be included.
%When distinguishing between rare subsets and noise can be hard.
In order to distinguish such rare subsets from noise, strong supporting evidence is required.
As illustrated in \Cref{chapter:kir} on qPCR data, prior evidence from \citet{Jiang:2012cf} supported existence of the rare 3-0 \gene{KIR3DS1}-\gene{KIR3DL1} copy number group at that sample size.
In hindsight, the expected copy number group frequencies obtained from \citet{Jiang:2012cf} could have been used as prior group frequencies for all copy number groups in the clustering of the qPCR data.
The prior information obtained from the qPCR dataset, was then used as training data in the next step of the analysis, in order to identify clusters predictive of \gene{KIR3DS1}-\gene{KIR3DL1} copy number in the SNP dataset.
%The \gls{knn} algorithm was used to classify unlabelled points, for which qPCR data was unavailable, from the vote of their K nearest labelled neighbours.
%The optimal value of K was selected by minimising the \gls{LOOCV}.

However, prior evidence only yields information about a cluster's expected proportion but little about its exact position and shape, which are usually experiment specific.
This can be due to the reliability of the instrumentation and sample preparation but also due to the stability of the biological sample, for example DNA variations changes can be seen over many human generations while variations in cell protein expression and cell populations fluctuate at a much shorter time scale.
%Supervised clustering can be really beneficial in the identification of rare subsets, since they may not always be visible in all samples.

Generally, in order for clusters to emerge, the number of events collected can be increased when possible, but this may also increase the within sample noise.


\subsection{ Within sample noise }

In flow cytometry, scatter channels for example, include many spurious events due to debris and cells clumping together creating doublets.
In fact, there are many technical and biological artefacts, in flow cytometry, which can lead to spurious clusters or outliers.
This is a problem because any clustering method or statistical test which relies on the mean estimation is potentially sensitive to outliers.
%An outstanding problem with supervised clustering for which I have not been able to find any satisfactory solution, is how to account for unlabelled data, or in the context of flow cytometry, nuisance clusters which are not included in the gating strategy.

One approach is to filter out these datapoints as is done in top-down hierarchical manual gating.
Automatic filtering of outliers usually relies on density estimation in order to exclude low density points from belonging to any cluster, as implemented in methods such as SPADE \citep{Simonds:2011jh}.
%and consequently all points can influence the gating.
%However there are methods consider all points in the dataset.

An alternative solution which does not rely on filtering, could be to define a background cluster, or since we are dealing with multimodal distributions, a sufficiently large number of background clusters to account for all clusters and then to filter out those which are not part of the study.
Certain model-based methods do this already by defining a background mixture with a covariance defined on the entire sample which essentially "mops up" all points which are not assigned with high posterior weight to any of the known components.
The \Rpackage{mclust} for example, allows one to define a noise component with an expected frequency.

%\section{ Identifying outliers } 
%While certain outliers may be clearly identifiable because they lie at the extreme of the instrument range, far from the rest of the data, other may be found with intermediate values and so are harder to spot.
%Very large intensity values are usually considered as noise, but low intensity values are harded to call with certainty.
% and require large samples sizes to confidently call as outliers.
%Others which lie within a more plausible range of values 
%There is a soft approach of dealing with outliers and a hard approach.

%A sample from a single flow cytometry experiment contains millions of events, many of which are discarded as part of the manual gating process.
%Some of these are real biological clusters but are ignored because they are not part of the cell populations under study:
%for example the first gate drawn is usually on the lymphocytes whereas the monocytes and granulocytes are often ignored.
%This is in part because they are no markers included in the experiment defined on these cell populations which would allow any further division.
%Other events are discarded because they are outliers.

Another approach to filtering outliers, as adopted by the \Rpackage{flowClust}, is to downweight the effect of outliers on the parameter estimation of the mixture model, by defining an outlyingness parameter which is inversely proportional to the Mahalanobis distance from a point to a cluster.  
Since the Mahalanobis distance is scaled by the covariance matrix, the distance is lesser to wider clusters than to tighter clusters:
\begin{equation} \label{equation:Mahalanobis}
(\boldsymbol{x_i}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x_i}-\boldsymbol{\mu})
\end{equation}
where $\boldsymbol{x_i}$ is the datapoint, $\boldsymbol{\mu}$ is the cluster mean and $\boldsymbol{\Sigma}$ is the cluster covariance.
%\[
  %u_{ig} = \frac{v+p}{v+(\boldsymbol{x_i}-\boldsymbol{\mu_g})^{\top}\boldsymbol{\Sigma}_{g}^{-1}(\boldsymbol{x_i}-\boldsymbol{\mu_g})}
%\]

It is worth noting that one issue with the Mahalanobis distance metric in detecting outliers, is that the outliers influences and greatly inflate the covariance matrix which is used in calculating the Mahalanobis distance.
To address this, packages such as the \Rpackage{robustbase}, use \gls{LOO} methods to identify outliers which have high leverage on the covariance estimation.
Another outlier metric, commonly used in linear regression is Cook's distance, which returns the leverage of every data point $i$  on the estimation of $\hat Y$ when the point is left out:
\[
D_i = \frac{ \sum_{j=1}^n (\hat Y_j\ - \hat Y_{j(i)})^2 }{p \ \mathrm{MSE}}
\]
where $p$ is the number of parameters in the model, $n$ is the number of datapoints, MSE is overall mean square error in the model, $ \hat Y_j $ is the predicted value for datapoint $j$ and $ \hat Y_{j(i)} $ is the predicted value when datapoint $i$ is left out of the model.

In higher dimensions, outliers are harder to identify as data tends to be sparse but also because they can be outliers without being outliers in any single dimension.
Reducing the dimensionality, using for example \gls{PCA} or \gls{viSNE}, and increasing the sample size should facilitate outlier identification.


\subsection{Between samples noise}


%A big challenge to the analysis of biological data, which greatly complicates identifying, matching and comparing clusters across multiple samples, is between sample noise.
Reproducibility is a big challenge in flow cytometry and biology in general.
As seen in \Cref{chapter:il2ra}, within-individual variation, ascertained from biological replicates, greatly compromises statistical power in detecting between-individual effects.
In flow cytometry for example, differences in cell treatment can lead to very different scatter patterns, as illustrated when comparing the scatter profile of the sample in \Cref{figure:il2ra-manual-gating-strategy} in \Cref{chapter:il2ra} to that of \Cref{figure:tony-cd4-gating} in \Cref{chapter:il2}.
%Staining in flow cytometry is notoriously noisy, so that
However, even when experimental conditions are kept constant,
%for example in flow cytometry using the same fluorochrome-antibody panel and the same \gls{PMT} voltage,
the shape and location of clusters across experiments can change.
%However, for a given panel and experimental protocol, one would not expect the clusters location to be comparable  to move much on the scatter channels since the morphological attributes of the cells are unlikely to be dependent on staining titration.
%Noise can be due to batch effects, for example staining discrepancies in flow cytometry, insufficient number of parameters to distinguish clusters, or because of sampling variation in rarer clusters.
Using normalisation, we can attempt to correct for these batch effects.
%Normalisation can facilitate this task by accounting for batch effects before clustering.

Normalisation involves first matching certain features of the data across samples, then transforming the data so that samples are comparable in a biologically meaningful way.
When identifying a fixed number of features across samples, such as modes in the density function, the first part of normalisation is similar to clustering and can rely on related algorithms.
%If k is unknown, the highest peaks can be selected across samples in the hope that these are the same across samples.
%Here normalisation is comparable to mode or bump hunting on the density function.
Such a method was used successfully in \Cref{chapter:il2ra} when gating bead data from different days, and again in \Cref{chapter:kir} to qPCR data for pooling data across different plates.

In \Cref{chapter:il2ra}, the linear transform applied to align the peaks of the bead data, was later applied to the identified clusters in the biological data, in order to make MFIs comparable across batches processed months apart.
%In \Cref{chapter:il2ra}, I showed that it possible to account for this time effect using beads.
%However as seen in \Cref{chapter:il2}, beads did not capture variation on a shorter time scale like seen in surface staining and in particular intra-cellular staining.

%Typically, normalisation is applied to univariate data, while clustering is applied to multivariate data.
%Meta-clustering which involves matching clusters across samples can also be considered to be a form of normalisation/clustering.
In \Cref{chapter:kir}, between sample normalisation was first applied to qPCR datasets to enable pooling, followed by joint calling.
I first identified the copy number peaks in the $\Delta$Ct of qPCR plates of the two most common groups.
The peaks were then aligned with a linear transform across plates
Pooling permitted the identification of the 3-0 group in \Cref{figure:Figure-1}.
In genotyping, normalisation is greatly facilitated by having matching probes across samples or spike-ins.
%On the other hand, in flow cytometry, there is much variation in the count of cells per sample.

In flow cytometry, there is a much larger number of cells than markers per sample, so clustering is typically done within each sample independently, followed by matching of clusters across samples.
%Normalisation can be done after the clustering to match and align the MFIs across samples.
%However, while this transform is appropriate for bead data which is not expected to change, it is obviously not appropriate when the MFI varies with genetic differences or ex-vivo stimulation.
%It can nonetheless be useful as a means to match populations across samples when the relative cell proportion is the parameter under study.
%Which is why in \Cref{chapter:il2ra}, beads are used to correct the sample MFI.
However, normalisation using peak alignment can also be done to align samples before the clustering, so that the same gating can be applied to all normalised samples, and allow for pooling of samples, to aid the identification of rarer cell populations \citep{Hahne:2009hl}.

Stability of stains is a big challenge and biologists often have an intuition of which markers are stable, and objectively, it does appear that certain stains are much more stable than others, be it due to the antibody specificity, the staining protocol or the thoroughness of the lab technician.
For example in \Cref{chapter:il2ra}, I was able to improve the repeatability of the CD25 MFI by correcting long-term fluctuations thanks to bead normalisation.
On the other hand, in \Cref{chapter:il2}, the variation in pSTAT5 MFI was not adequately captured by beads, possibly due to titration issues, so I had to resort to various other normalisation approaches, none of which performed particularly well.

%and become harder to call when the number of dimensions increases (they are not linearly separable in all dimensions).
In the context of flow cytometry data, the manual gate hierarchy contributes prior information about the expected relative frequency of the different types of cells and their relative marker expression, but their absolute marker expression is generally not readily comparable across samples and requires normalisation as shown in \Cref{chapter:il2ra}.
%This is why thresholds are often used to dichotomise the data into negative and positive subsets, however the problem remains of where to adjust the threshold per sample.

From my experience, normalisation of raw flow cytometry data using peak identification is as hard a problem as clustering because of the multimodality and the level of noise in these data.
In \Cref{chapter:il2}, I found that the peaks could not be identified reliably using a sliding window approach.
%Choosing the right window-size parameter for peak finding algorithms is channel specific and not trivial.
Also this type of univariate clustering of each flow marker independently does not exploit the correlation which exists between markers.
Finally, mismatching of peaks in the alignment is more detrimental to repeatability than not doing any normalisation as the wrong clusters will be aligned.

While in genetic data, the number of probes across arrays is constant and identifiable, flow cytometry data can contain very different number of events between samples and the distinction between cell population is often blurry.
Normalisation methods applied to flow cytometry data must thus account for the sampling variation as well as staining discrepancies.
Distinguishing between staining noise and actual differences in cells biology requires a certain level of prior knowledge which is context dependent and difficult to implement algorithmically.
%Subsequently, a sample from the same individual taken and analysed on a different day, can have a very different profile.

\subsection{ Small number of samples }

Typically, immunostaining flow cytometry experiments contain much smaller number of samples than case-control experiments because on one hand, samples are a limited resource and on the other, sample preparation and running tubes on the flow cytometer are an expensive and time-consuming process.
Also, experiments undertaken in flow cytometry are often pilot experiments or tubes ran to test and optimise panels, so not always complete datasets.
These pilot experiments, may be implemented with varying degrees of thoroughness and hence generally poorly comparable.

When dealing with the relatively small numbers of samples available in flow cytometry, data quality is more influential than the methods used to process the data: sophisticated methods are no replacement for good data.
However, being able to make the judgment call between good and bad data necessitates understanding of the experimental context and the underlying biology, for example what cell populations to expect and their relative position and frequency.
In genotyping, calling a genotype based on only a few samples is sometimes possible, because the absolute position of the signal clouds can be estimated thanks to the stability of the DNA molecule, the standardisation of DNA preparation protocols and of SNP arrays \citep{Di:2005uj,Giannoulatou:2008ty}.
%Furthermore the population frequencies of many variants have been estimated in previous studies, providing some prior of what proportions of wild-type, heterozygous and homozygous, to expect in a new sample of unrelated individuals drawn from the same population.
On the other hand, in flow cytometry, the average protein expression of cell populations, as measured by the \gls{MFI}, is generally not directly comparable across experiments, so it is difficult to predict where a cell population will fall.
Instead, as is done when sorting cells, a few events need to be collected first to get some estimate of where to draw gates.
Also, the frequency of cell populations can differ widely between individuals, and the ratio of cell populations, for example naive to memory, changes in the course of an individual's lifetime with exposure to environment. 
However, clusters can be defined in relation to one another, for example memory cells are lower for \protein{CD45RA} than naive cells but lower in CD25 (\Cref{chapter:il2ra}).

This prior knowledge is acquired through the experience of having seen a large number of samples and difficult to encode in an automated method.
Processing in larger batches or perhaps reducing the human element in flow cytometry could be a first step towards automation.
%http://www.aber.ac.uk/en/cs/research/cb/projects/robotscientist/
This will become a necessity as the number of samples grows.

%Futhermore the fluorochrome antibody mix used per tube costs in the order of \textsterling20
%the method of operating flow cytometer is still quite manual in most labs.
However there are facilities, for example at the Vaccine Research Center at the National Institute of Allergy and Infections Diseases, where these processes have been automated with robotics and consequently may run hundreds of samples a day.
Automatic methods are more pervasive in those labs since manual analysis is no longer a viable option.
%Within our lab, samples from longitudinal experiments are more common and come in over a long period of time and need to be analysed on the day or frozen.
Presently there are few flow datasets sufficiently well-powered to detect the effect sizes seen in complex diseases.
%Furthermore, normalisation beads or controls are not consistently used which complicates analysis.


\section{ A plan for moving from manual gating to automatic gating }

Despite the clear advantages that automatic gating promises over manual gating, due to the level of noise possible in flow cytometry experiments and the small sample sizes, fully automated gating may not always perform well.
%Various cell preparation or staining artefacts in flow cytometry can make cell populations indistinguishable, and beyond a certain threshold of uncertainty, a sample yields little information.
%Spillover introduces artificial marker correlation and can increase or decrease the fluorescence intensity of cell populations.  
Perhaps the immediate goal of automatic shouldn't be to supersede manual gating but instead to complement it.
Until the number of samples is sufficiently large, these methods can benefit from the prior knowledge that manual analysis contributes.
Here I will present steps to be taken towards using these more targeted approaches routinely.

\subsection{Standards}

Standards are crucial to improving reproducibility.
Since FCS files does not contain sufficient metadata to understand the context of the experiment, the name of the FCS file is typically used to map the sample back to the donor in order to retrieve covariates such as disease status, age, sex or genotype.
When the naming and documentation is incomplete this makes it very hard to automate the analysis of these data.
Some guidelines have been set out by \citet{Lee:2008ed} in an attempt to incorporate the minimum information about a flow cytometry experiment (MIFlowCyt).
This involves the experiment overview, which includes the purpose of the experiment, the experiment variables, conclusions and the quality control.
Also information about the sample, the source, the material used, the treatment of the cells and the reagents.
Instrument details and configuration details.
%Instrument identification Fluidics configuration
%Optical configuration
%Electronic configuration
%Data analysis
%List-mode data
%Compensation
%Gating
%Descriptive statistics
Dealing with dealing with typos and inconsistencies in these file names, is in my experience very time-consuming and distracting from the analysis.
Also channel names as given in the FCS files are not always consistent across experiments.
When the FCS file do contain metadata it does not always match the naming of the file.
For example the data as stored inside the FCS file did not match the date given in the filename.
In particular, FCS files do not contain sample identification information, which makes matching back to genotypes cumbersome and error-prone.
%Some minor issues for example, when experiments are done over night the dates might mismatch


\subsection{Finding common ground}

Analysing flow cytometry data has brought to light the many issues surrounding the division of skill and difference in thinking between data generation and data analysis.
One obstacle to standardising generation and analysis may be the disagreement on which protocol is the best to follow.
However, inconsistencies as trivial as naming conventions may waste precious man hours for the person analysing the data or even to the person who generated the data returning to it after a long while.
I believe, part of the solution is to involve the person generating the data in the analysis, so that they can appreciate the implications.
Another part of the solution is to encourage automation of these more tedious tasks, as is being done in certain labs which use robots to feed the flow cytometer and barcoding to name the samples.
%At least automation is consistently and predictably wrong.

On the other hand, as as statistician I also need to have an appreciation of the quality of the underlying technology and the purpose of the experiment.
For example, in flow cytometry, one needs to be aware that a large number of events are debris of no biological interest and can be excluded based on side and forward scatter.
Similarly certain patterns may be simply from staining artefacts, sample preparation (permeabilisation) or correlation of flow markers due to spillover.
%as was argued in chapter 3

While I am sure these observations are not only specific to flow cytometry analysis, I would argue they are more striking because of the flexibility flow cytometry offers both in terms of generation and analysis, and how little is known of the underlying cell populations.

In order to encourage the use of more rigorous computational gating methods, these tools need to be made more accessible to biologists and non-programmers.
For example, a first step could be to use the manual gates but to allow them to move with the data.
I think completely removing the two dimensional visualisation for example might be too alienating to some.
There is also a growing need for non-proprietary software which integrates well with the manual analysis.


Realistically, the move to automation is likely to be incremental, for example by replacing the sequence of univariate or bivariate gating steps in the process.
As was seen in \Cref{chapter:il2ra}, the one-dimensional sequential top-down gating strategy can easily be coded up as an algorithm using mixture models or bead-derived thresholds.  


One way this can be achieved is by extending FlowJo analysis.
FlowJo is the main tool used by immunologists for identifying groups of cells in flow cytometry.
The unit of work in FlowJo is the workspace in which FCS files are first loaded and then gated.
The workspace also saves the cell populations statistics which need to be updated when the gates move.

\subsection{Extending FlowJo analysis}

Unfortunately, parsing FlowJo workspaces in order to extract manual gates is not straighforward because the format is poorly documented and not stable between releases.
Although there are several BioConductor packages designed to import and parse FlowJo workspaces, \texttt{flowUtils}, \texttt{gatingML}, \texttt{flowJo}, \texttt{flowWorkspace}, I have generally found that the R/FlowJo interface was not very reliable: for example \texttt{flowWorkspace} parsed the workspace without errors but the returned statistics did not match the ones returned by FlowJo.
%This is probably why \contributor{Vincent Plagnol}
An alternative would be to develop a bespoke XML parser to extract gates from FlowJo workspace files, but this approach is laborious as it requires in-depth knowledge of the FlowJo XML schema which changes on each new release.
%Additionally, I found the gate coordinates in FlowJo to be imprecise when plotted in R because FlowJo applies its own binning on the data.
%In fact, I found a serious bug in FlowJo: just loading an FCS file into FlowJo and reexporting it changes the data as some sort of binning is applied.
In the end, I found the best solution was to export CLR files which are simply the classification results from FlowJo.
Unfortunately I was met with another FlowJo hindrance: instead of exporting these files in a memory-efficient compressed binary format, FlowJo exports them as text which results in very large files and makes exporting of all clustering results impractical. 
FlowJo crashed or hanged on numerous occasions when trying to accomplish this simple task.
Hence I resorted to exporting only a few CLR files from which I could estimate the gate coordinates.
In order to obtain gate coordinates from the CLR format, the dimensions on which the gate is defined need to be stored as part of the column name.
One method of approximating gate coordinate is to calculate the mean and covariance of a CLR cluster and to use the Mahalanobis distance, hence approximating the cluster with an ellipse.
However, it does simplify the process to know in which dimensions the gate was defined.


\subsection{Alternatives to FlowJo}

An R only substitute to FlowJo for manual gating is to draw polygons on the R display using the \Rfunction{locator} and then using the \Rfunction{in.polygon} to extract points which fall within the polygon.
This is an approach that I used in \Cref{chapter:il2} to emulate \contributor{Tony Cutler}'s gating.


There also exists some commercial alternatives to FlowJo  such as \texttt{ADICyt}\footnote{\url{http://www.adinis.sk/en/products/bioinformatics-and-data-processing/adicyt.html}} and \texttt{Infinicyt}\footnote{\url{http://www.infinicyt.com/}}, but at the time of writing, these are not nearly as widely used as FlowJo.
In spite of these commercial alternatives, I still believe there remains a gap in the market for a new open source and extensible piece of software which reconciles manual, supervised and unsupervised flow cytometry analysis, and provides further multidimensional visualisation techniques.

The established manual gating method involves obtaining gate coordinates from drawing gates in FlowJo and applying these same gate coordinates across samples.
Manual gates can also provide initial starting parameters to a clustering algorithm, by first calculating the mean and the covariance of the gated populations, and then applying an \gls{EM} algorithm, to refine the parameters to better fit the data.
%assuming an elliptical gate
This is the approach taken by X-Cyt \citep{Hu:2013bg} which uses manual gates as templates.
FlowJo also provides a basic version of this feature known as "magnetic gate" that moves gates to accommodate the maximum number of events.
I also tried a similar approach in \Cref{chapter:il2} since manual gates were not available for all samples.
First, I let the mean of the ellipse be influenced by the data while the covariance was set as fixed.
I then obtained a classification by defining a threshold on the Mahalanobis distance (\Cref{equation:Mahalanobis}) above which points were excluded.
This worked reasonably well unless there was too much overlap with another cluster, in which case the gate was pulled by the wrong cluster.

When several samples have been manually gated, the gate mean and covariance from each sample can be used to define the mean and covariance hyper parameters of the priors in the mixture model to guide the parameter estimation, as implemented in the \Rpackage{flowClust} for example.
I found this approach worked quite well when gating CD4\positive lymphocytes on forward, side scatter and CD4.
%If several samples have been manually gated then these can be incorporated in the definition of the prior on the parameters of the mixture model.
%FlowClust as opposed to Mclust offers the option of defining per component priors.
%This threshold can be defined by using a chi squared distribution in the one dimensional case.

This is the motivation behind the \Rpackage{openCyto}.
The automated methods need to complement the manual methods for now, so that the change from manual to automated happens gradually.
On the other hand by continuously benchmarking automated analysis against manual, we are not exploiting the true power of automated algorithms which is to teach us new biology or to put back into question our hierarchical view of immunology.
As an example I ran flowClust unsupervised with a large number of clusters and then picked the cluster which gave the best association with each SNP.
This is an idea which was explored with flowMeans \citep{Aghaeepour:2010fv}.
The issue however is the metaclustering step of matching clusters across samples is not trivial especially in the presence of batch effects.

%Therefore clustering is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and error.  
%It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.  
%Clustering can also be viewed as a latent variable problem where the cluster labels are considered to be the missing data.

%Lessons learned
%Things I have learned: philosophical discourse


\section{Conclusion}

Larger datasets have allowed us to see finer biological variation both in genotypes and cell subsets than previously possible.
However, sometimes taking a different view of the same dataset, by doing a different kind of experiment, for example qPCR, or adding parameters, for example additional markers in flow cytometry, can help uncover patterns which might not have been visible even at larger sample sizes.
For example in \Cref{chapter:kir}, qPCR allowed us to discover a SNP predictive of \gene{KIR3DS1}-\gene{KIR3DL1} copy number which we would have not found even at larger sample sizes.
Furthermore, even without doing further experiments, analytical methods such as unsupervised clustering may discover new features.
For example in \Cref{chapter:il2}, unsupervised clustering algorithms analysing pSTAT5 response at different doses of proleukin uncovered previously ignored responsive subset of cells.

Although large datasets can support methods with larger number of parameters as they are less prone to overfitting,
it is easy to fall into the trap of applying over complex analytical methods to account for all the intricacies of the data,
when in practice, simpler methods perform nearly as well and are much faster and easy to implement.
Simple methods can also be combined to reach a consensus and this popular machine learning approach known as boosting increases performance, but often at the expense of interpretability.

Interpretability is perhaps one of the more important issues in order to encourage biologists to use these computational methods.
While mixture modelling approaches are conceptually close to manual gating, probabilistic cell types are not always intuitive to biologists, so their true power cannot be fully exploited as we end up applying hard cut-offs (see \Cref{chapter:il2ra}).
Biologists enjoy manual gating becomes it gives them the freedom to draw arbitrary exclusive gates whose contour can be made arbitrarily complex.
This freedom however comes at the cost of exacerbating the disagreement in standards and definitions in immunology.
While discrepancies in gate positions are unlikely to make much of an impact on the MFI and relative proportion of common cell populations, they can make a big difference on rarer cell populations such as regulatory T cells were the effects are much smaller.

%This can occasionally be achieved by centering the ellipse on the mean of the datapoints which fall within the gate.

While, in my opinion, automated clustering ought to be applied more widely in flow cytometry data analysis, data analysis can still be done manually as the number of samples and parameters are still manageable.
However, as the number of samples and parameters continue to grow, biologists will need to resort to fully automated method, hence it is important that we continue developing these methods.
These methods will require some level of expertise and decent visualisation to guide and reassure the user.
Although, over-reliance on visualisation can mislead the analysis of high-dimensional because clustering is always projected back to two dimensions or linear combinations of dimensions.
The automatic gating of flow cytometry community is growing stronger with a lot of contributions to BioConductor and the GenePattern web interface from the Broad Institute.
In particular two labs, Raphael Gottardo at the Fred Hutchinson Cancer Research Center in the USA and Ryan Brinkman at the Terry Fox Laboratory in Canada, have been central in developing automatic gating software and bring together the automatic gating flow cytometry community as part of the FlowCAP challenge every year \citep{Aghaeepour:2013dg}.

% I admit there has been a lack of simulations but flow data is hard to simulate.
% it's not just simply labelled data that can be permuted and anyway the datasets are too large for this approach to be practical
% simulate small responsive population?
%On sufficiently large datasets, all methods tend to be equivalent.
%In a Bayesian setting, as the datasets are growing larger, the likelihood computed on the data will have a much stronger influence than the prior.


%Methods should be simple but no simpler.
%Our mission should not be to foreseee the future but to enable it.  

%How much we chose to automate depends greatly on the experiment but also more pertinently on the volume of data generated.
%However poor staining or instrumental configuration can lead to unexpected distributions.
%In \Cref{chapter:il2}, the permeabilisation protocol made the staining noisy, so that the MFIs were not reproducible.
%But different computational tools could be used to highlight different populations.

The benchmarking problem remains, however, since in science as opposed to engineering, we do not know exactly what the outcome of the process ought to be.
%Philosophically this is perhaps what still makes data analysis an art, is that there is still some mystery since we haven't discovered all the rules yet.
If our benchmark is comparison to manual gating then clearly no method can ever outperform it.
Independent benchmarks such as repeatability are fairer, but perhaps a more general benchmark could be the utility of the clustering outcome.
For example, whether it is predictive for diagnosis or if it correlates strongly with genetics or some other covariate under study.
Of course we will need to account for multiple testing given the large numbers of cell populations which can tested using this method \citep{Roederer:2015eu}.

In my opinion, two aspects of flow cytometry analysis which are worthy of uther work are normalisation and selection of an optimal transform.
Both of these could be included as part of the clustering step.

Finally, my view is that we will only reap the true fruits of automated analysis once the number of samples has grown considerably, although this might require earlier steps in the process, such as the processing of samples, to be further standardised and automated first, so that the signal may rise above the noise.

%We might even see Cytobank accumulate samples like ArrayExpress.
%Finally, to conclude on a more philosophical tone from on my favourite french authors:

%Despite huge technological advances flow cytometry is still a very noisy technology and while bespoke analysis methods can be developed, larger sample sizes may be necessary to overcome the multitude of unwanted batch effects ranging from antibody stickiness to sample preparation.



%\begin{center}
%\vspace*{\fill}
%{\fontfamily{frc}\selectfont \foreignlanguage{french} {
%L'avenir n'est jamais que du présent à mettre en ordre, tu n'as pas à le prévoir, mais à le rendre possible.
%%Pour ce qui est de l'avenir, il ne s'agit pas de le prévoir mais de le rendre possible.
%}}
%
%-
%{\fontfamily{frc}\selectfont \foreignlanguage{french} { Antoine de Saint-Exupéry }}
%\vspace*{\fill}
%\end{center}
%









