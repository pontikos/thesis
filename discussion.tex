\chapter{Discussion}

\section{Reproducibility}


Reproducibility in flow cytometry is challenging. 
Subsequently, a sample from the same individual analysed on different days can have a very different profile.
As seen in \Cref{chapter:il2ra}, this within-individual variation greatly compromises statistical power in detecting between-individual effects.

In long-running experiments when samples are analysed over a period of months, some noise may be attributed to
variation in instrument sensitivity.
In \Cref{chapter:il2ra}, we showed we can account for this using beads.
%To correct for day variation in  beads can be used to account for \citep{Dendrou:2009bl}.
However beads do not capture variation on shorter time scale or due to batch effects.
Staining and in particular intra-cellular staining, of internal markers such as STAT5 and FOXP3, is subject to batch and day variation.

Another approach is to align the peaks of the univariate distribution in the whole sample as was done for qPCR $\Delta$Ct values in \Cref{chapter:kir}.
However, in the flow cytometry samples we studied, we found that the peaks cannot always be identified reliably and choosing the right window-size parameter
for peak finding algorithms is channel specific and not trivial.
Also mismatching of peaks in alignment are more detrimental to repeatability than not any doing normalisation.


\section{To move data or to move gate: normalisation vs clustering}

%Sometimes it is better not to normalise and normalisation may hide the true biological effect.
The question really boils down to whether we should normalise the samples or cluster each sample separately.
Normalisation is effectively analogous to meta-clustering since we are attempting to match cell populations across samples.  
Normalisation facilitates matching clusters across datasets but in doing can remove meaningful biological variation.  

Theoretically if the data was perfectly aligned across days then gates should not be moved.  
Threshold gates should not move unless they are relative to some internal control.
The cell phenotypes which are usually measured are the mean intensity of a particular channel, or the percentage of cells.
Depending on the shape of the distribution these can be more or less sensitive to the position of the gate.

Normalisation removes unwanted experimental variation to make data comparable even when collected on different days,
proccessed with different protocols or analysed with different instruments.
However distinguishing between what is unwanted and what is meaningful variation relies ultimately on a bias-prone judgement call.

As we have seen, different normalisation approaches make different assumptions about what is unwanted variation and the shape of the data.
The actual choice of normalisation depends on the characteristics of the data we wish to compare.

We have only considered the univariate density function here but identification can be extended to the multivariate case.

Next let's consider normalisation of multivariate distributions.  
The purpose of principal component correction is to remove unwanted correlation between samples.

When applying a linear transform to multivariate data the correlation between the dimensions is preserved since the multiplicative factors cancel out.
However the covariance changes.
Care must be taken when applying a non-linear transform.  

The samples are related and we wish to exploit this structure without forcing data to be perfectly comparable.
A less biased approach is to allow for different cluster location and shapes across datasets and instead incorporate the clustering in some sort of hierarchical framework.  
Normalisation or meta-clustering is a necessary step when data is pooled across studies.

One issue with normalisation based on peak alignment is that the level of there is more noise variation than biological variation between samples so that in certain samples,
so much so that in certain samples the peaks are only identifiable after preliminary gating of lymphocytes.



%Therefore clustering is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and error.  
%It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties.  
%Clustering can also be viewed as a latent variable problem where the cluster labels are considered to be the missing data.

\section{Supervised clustering}

On the KIR dataset, we used prior information about the copy number in order to identify clusters in the SNP dataset.
In the flow data, the manual gates could be used as prior information to guide the automatic gating.
For example in flowClust, the manual gates can be used to define the priors in the mixture model.

\section{ Method of determining phenotype affects power to detect genetic association }


Consistency is an important factor in identifying clusters.
%Decomposing the variance into tech

\section{ future application of methods }

% experiment vs analysis: the great divide?
Perhaps the geatest obstacle to standardising analysis is our inherent dislike of standards.
What motivates an experiment is may be a moment of inspiriation and leads to the generation of flow data is often an adhoc thought process,
which is implemented with different degrees of thouroughness and method.
However, simple things like naming conventions waste precious man hours.
To err is human,  which is why automation is necessary.

On the other hand, statisticians need to have an understanding of: the underlying technology, the purpose of the experiment.
For example, in flow cytometry, a large number of events are debris of no biological interest and some patterns are just artefacts of the staining
or the experimental protocol.

A lot of my work unfortunately has involved dealing with typos and inconsistencies in file names and channel names.




