% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
This thesis develops a broad class of models, useful for learning and forecasting in such domains as time series, geological formations, and physical dynamics.
These models are based on Gaussian processes, which can express a wide variety of statistical structure, depending on the choice of kernel.
%The types of generalization \gp{}s can perform depends on the kernel, whose 
Historically, the type of kernel has been chosen by hand by experts.
%Choosing the type of kernel typically requires trial-and-error by experts, 
%limiting the power of Gaussian processes in practice.
In this thesis, we show how to automate this task, creating an artificial statistician capable of systematically exploring a large space of models.

The introductory chapters show many types of structure, such as periodicity, changepoints, additivity, and symmetries can be encoded by kernels, and that these structure can be combined by combining kernels.
For example, compound kernels can produce priors over topological manifolds such as cylinders, toruses, and M\"{o}bius strips, as well as their higher-dimensional analogues.

The main contribution of this thesis is to show how this open-ended space of models can be explored automatically.
To do so, we define a simple grammar over kernels, a search criterion (marginal likelihood), and a breadth-first search procedure.
Combining these, we present a procedure which takes a dataset, and outputs a detailed report with graphs and automatically-generated text illustrating the qualitatively different, and potentially novel, types of structure discovered in that dataset.
This system automates parts of the model-building and analysis currently performed by expert statisticians.

%Kernel learning is a type of feature learning, a problem which has received intense interest in the last decade.
%In particular, the practical success of deep neural networks has demonstrated the potential of automatic feature extraction.

This thesis also explores several extensions to Gaussian process models.
First, building on earlier work relating Gaussian processes and neural nets, we explore the natural extensions of these models to \emph{deep kernels} and \emph{deep Gaussian processes}.
Second, we examine the model class consisting of the sum of functions of all possible combinations of input variables.
We show a close connection between this model class and the recently-developed regularization method of \emph{dropout}.
Third, we combine Gaussian processes with the Dirichlet process to produce the \emph{warped mixture model} - an unsupervised clustering model with nonparametric cluster shapes, and a corresponding latent space in which each cluster has an interpretable parametric form.
\end{abstract}
