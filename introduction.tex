%\input{common/header.tex}
%\inbpdocument

\chapter[Introduction]{ \label{chapter:intro} Introduction}

Over the last twenty years, the size and scale of biological analyses has increased at a tremendous rate.
As these datasets continue to grow, we are continuously interesting in discovering patterns or trends in the data
which may influence clinical variables, such as disease progression or status,
but also at a gentic variables, in the hope of understanding new biology.

Although the human brain is finely tuned for pattern recognition, it is perhaps not the most objective classification system
and very good at dealing with uncertainty, and it is certainly not the fastest.
Furthermore since the data collection technology is becoming massively parallelised
and more and more parameters can be concurrently measured, the scale of these datasets has long surpassed manual analysis.

In this thesis we will be dealing with some genetic and flow cytometry datasets.
The technology that revolutionised genotyping, is SNP genotyping which probes concurrently hundred-thousands of genomic loci within an individual and has parallelised
to thousands of individuals.
Another important tool to immunologists or any researcher studying large volumes of cells,
which is the main method studied in these thesis, is flow cytometry, a high-throughput technology which can currently measure up to twenty cell
parameters in millions of cells in the space of a few minutes.
In the future, high througput sequencing and massy cytometry will generate larger numbers of parameters at a higher rate.

However when datasets are generated independently, even when on the same sample, the results might not be directly comparable because of
instrument, experimental variation and variation in the analyses.
Also the comparison methods needs to be defined and what is a meaningful biological comparison?
These two related problems are addressed by normalisation and clustering.
As in all statistcal anlyses, some prior knowledge is required explicitly or implicitly defined.

In this thesis, I will focus on normalisation and clustering in cell-level parameters, acquired with flow cytometry,
but also genetic data acquired from qPCR and SNP chip.

Throughout the thesis the main themes will be normalisation and clustering.

In the first chapter, I will revisit a large, long-running flow cytometry experiment.
normalisation using external controls, fluoresecent beads, then
flow can we replicate the analysis done by a human?
I will also assess the influence of clustering method on association testing.

In the second chapter,
Using normalisation and clustering, joining datasets from different experiments, qPCR and SNP, to discover new patterns.

In the following chapter,
we will once more consider normalisation 
Can we discover novel clusters



\section{Biology of type 1 diabetes}

\subsection{Etiology and diagnosis}

\Gls{T1D} (OMIM:222100), also known as insulin dependent diabetes mellitus (diabetes - \foreignlanguage{greek}{diab\'hths},
a passer through, and mellitus - \foreignlanguage{greek}{m\'eli}, honey), is a disease reported as early as $1500$ BC \citep{Poretsky:2010wr}.
It holds its name from the characteristic symptom of excessive discharge of high-glucose urine (glycosuria or hyperglycemia-induced osmotic diuresis polyuria).
It has since been established that this symptom is the consequence of persistently high levels of glucose (hyperglycemia) in the blood due to an insufficiency in insulin,
the hormone responsible for glucose regulation.
Long term high-glucose levels lead to dehydration, drowsiness, cardio-vascular complications, increased chances of morbidity and death.  
If left untreated T1D is a debilitating and life-threatening disease.

From post-mortem analysis of pancreatic samples and animal models,
it is widely accepted that the cause of the insulin deficiency in \gls{T1D} is an autoimmune reaction
in which insulin and insulin-producing $\beta$-cells of the pancreatic islets
are progressively destroyed primarily through auto-reactive T cells \citep{Todd:2010bl}.  
%, a hormone essential in regulating blood glucose levels, which causes the clinical symptoms associated with T1D:

In the last 50 years, the number of cases of T1D worldwide has increased and is predicted to continue increasing in the next decade,
affecting mainly children under the age of 5 \citep{Patterson:2009gj}.
%T1D manifests itself typically under the age of 20 which advocates a disease with a strong genetic predisposition.
The World Health Organization reported that in August 2011 around 34 million people worldwide were diagnosed with T1D.
%Glycaemia and diabetes are rising globally, driven both by population growth and ageing and by increasing age-specific prevalences.
%and affects around 5\% of the UK population \cite{Levy:2011wz}.

At present there is no cure for \gls{T1D}.
%T1D is diagnosed by screening for consistently high levels of glucose in the blood at which point
The only existing treatment is the regular intravenous administration of exogenous insulin.  
%This treatment is and there are efforts in developing more convenient method of delivering of insulin.
Pre-symptomatic detection of T1D relies on testing for presence of auto-antibodies against insulin and its precursors.
Early detection of T1D allows a better understanding of how the disease progresses and how we can develop therapies to delay its onset,
reduce the symptoms and hopefully in the future, cure the disease.  

One such therapy currently undergoing clinical trials in our lab attempts to restore immune tolerance to pancreatic $\beta$-cells with low-dose \Gls{IL-2},
in newly diagnosed \gls{T1D} patients.
%I will be presenting my preliminary analysis of these trials later in my thesis.
%Effective preventive interventions are needed, and health systems should prepare to detect and manage diabetes and its ramifications \cite{Danaei:2011hg}.

%(and hinder ketonacidocis)
%There are numerous genetic markers which increase susceptibility to T1D but these only indicate a pre-disposition to the disease.  

\subsection{Heritability}

%T1D is an auto-immune disease whereby cells of the immune system target and destroy the body’s own insulin secreting cells of the pancreas, the $\beta$-cells.

%Every trait is the result of a genetic and environmental influence and interaction.
Patterns of familial clustering suggest that some portion of T1D risk is genetically inheritable.
However since families also tend to share environment, it is difficult to decouple the genetic from the environmental effect.
%The genetic heritability of a trait independent of environment is known as the narrow sense heritability.
%Environment heterogeneity can lead to confounding and underestimation of heritability
One way of assessing the genetic influence relative to that of the environment is from the T1D concordance rate in \gls{MZ} and \gls{DZ} twins.
MZ twins have near identical \gls{DNA} whereas DZ twins, like other siblings, share on average half of their DNA.
But since \gls{MZ} and \gls{DZ} twins both tend to share environment, then the ratio of T1D concordance in \gls{MZ} twins ($\lambda_{MZ}$) 
over that in DZ twins ($\lambda_{DZ}$) should be indicative of genetic risk independently of environment \citep{Clayton:2009kf}:
$ \lambda_s = \frac{\lambda_{MZ}}{\lambda_{DZ}}$,
%An alternative measure of heritability is the ratio of incidences between monozygotic and dizygotic twins 
%\approx 5$
%The first measure of asssessing the genetic influence is the concordance rate.
%Under the age of 10 the concordance rate in MZ twins is good but after that age we have to wait longer for the second twin to also display symptoms.
\citet{Hyttinen:2003kn} estimated the pairwise concordance rate of $\lambda_{MZ}=42.9\%$ and $\lambda_{DZ}=7.4\%$ in 44 MZ and 182 DZ twins,
yielding $\lambda_s = \frac{42.9}{7.4} = 5.8$.
However, this could be an understimate since in the long-term follow-up study in 83 MZ twins, \citet{Redondo:2008} found that $\lambda_{MZ}=65\%$.
%and the auto-antibody concordance is $78\%$.  
This approach is extendable to relatives of type $R$ by considering the increase in risk $K_R$ in an individual when a relative of type $R$ has the disease compared to the population risk $K$.
%\citet{Spencer:2011be}
\citet{Risch:1987wm} defines this as the relative recurrence risk: $\lambda_R = \frac{K_R}{K}$.
One drawback of this approach is that more distant relatives tend to share less environmental factors which makes decoupling environment from genetics harder.
\cite{Risch:1987wm} estimated $K=.004$ and $K_R=.06$, yielding $\lambda_R=15$.
This confers a huge genetic risk but is likely to be an overestimate as the population risk is closer to $5\%$.

%The sibling recurrence risk is the probability that a sibling of an affected individual is also affected.
%The sibling relative risk which is the ratio of the sibling recurrence risk and the overall disease prevalence, is used by geneticists in planning and evaluating studies aimed at discovering genes conferring susceptibility to disease.

%$\lambda_s = \frac {Pr( Y_j=1 | Y_k=1)} {Pr( Y_k=1 )} $

%For T1D the population risk is about $5\%$ so $Pr( Y_k=1 ) = .05$
%The age of diagnosis concordance is highest in MZ twins when one develops T1D under the age of 10 \citep{Redondo:2008} which suggests as form of diabetes with a stronger genetic risk.
%Past the age of ten, \citet{Redondo:2008} find we can wait up to 43 years for the second MZ twin to also develop T1D,
%which suggests a form of T1D where the genetic risk is lower but instead more susceptible to environmental triggers.
%The early age of onset is a clue as to how strong the genetic effect in relation to the environment.

%Also the time between autoantibody positivity and T1D diagnosis (low insulin) increases with age.
%maybe this is because we have more functioning insulin circulating

%This shows that even against an identical genetic background, the concordance is not $100%$ which illustrates the growing influence of environment and gene-environment interaction in lower risk twins.
%However, within families, the disorder follows no clear mode of inheritance and is generally thought to result from the combined effects of multiple genes interacting with non- genetic factors (Risch 1987; Thomson et al. 1988; Rich 1990).

\subsection{Genetic architecture}

Having established that there is a considerable genetic predisposition to T1D, we are interested in identifying likely causal variants in our genetic code which might lead
some insights onto the mechanism of the disease.
%We are interesting in discovering how many genes have an effect

Since insulin is a target of the autoimmune response in T1D, the insulin gene \gene{INS} was tested as a strong candidate region and was found to associate with
the disease \citep{Metcalfe:2001}. 
\citet{Metcalfe:2001} found that within 40 \gls{MZ} twin pairs concordant for T1D, $87.5\%$ carried the high risk \gene{INS} variant (Hph I),
compared to $59.5\%$ in 47 discordant MZ twins.


Linkage studies study based on the recombination of multiallelic genetic markers in families affected by T1D,
first mapped a genetic risk factor to the HLA class II region on chromosome 6.

%According to \citet{Alper200689}, in the $6\%$ of sibs of a patient which are concordant for T1D, $16\%$ are MHC identical.
%However in concordant MZ twins, only $33$ to $42\%$ share the same variant.  This suggests that other genes are important.

%\paragraph {Linkage Studies}

%If we take into account the cumulative genetic risk and assume an additive model of risk

However, the risk variants detected by linkage studies only explained a percentage of the heritability,
so it is clear that T1D must involve smaller effect size risk variants that are only detectable at sample sizes larger than those achievable by linkage studies.

%\paragraph {GWAS}

\Glspl{GWAS} use high density \gls{SNP} arrays, such as the GeneChip 500K Mapping Array Set (Affymetrix chip),
to test the association of polymorphism in single basepairs, \glspl{SNP}, across the genome with disease or other traits,
in thousands of unrelated cases and controls.


\Gls{GWAS} have confirmed strong association of T1D within the \Gls{HLA} loci (chromsosome 6p21) as well as 50 other loci including notably regions near
\gene{INS} (chromosome 11p15), \gene{CTLA4} (chromosome 2q33), \gene{PTPN22} (chromosome 1p13), \gene{IL2RA} (chromosome 10p15)
and \gene{IFIH1} (chromosome 2q24) \citep{Burton:2007hta,Barrett:2009jq}
(\url{www.t1dbase.org}).

Within the HLA region, the strongest effect comes from HLA class II loci, \gene{HLA-DRB1} and \gene{HLA-DQB1},
but there is also an independent effect from HLA class I loci, HLA-A and HLA-B \citep{Howson:2009bl}.
The HLA class II genes code for membrane-bound proteins which expose extra-cellular antigens to T cells,
%it is hypothesized that they may play a role in the insulin presentation pathway.  
whereas the HLA class II, expose fragments of the cell's internal peptides to its surface for inspection by immune cells such as \gls{NK} cells.

%IL2RA codes for the alpha chain of the IL2 cytokine receptor (better known as CD25), are situated outside of the HLA region.
%Within the intronic (potentially regulatory) IL2RA region, three single nucleotide polymorphisms (SNPs) have shown to be significantly associated in case-control studies (Lowe et al., 2007; Smyth et al., 2008; Maier et al., 2009)

%IL2RA codes for the alpha chain of the IL2 cytokine receptor (better known as CD25), are situated outside of the HLA region.
%Within the intronic (potentially regulatory) IL2RA region, three single nucleotide polymorphisms (SNPs) have shown to be significantly associated in case-control studies (Lowe et al., 2007; Smyth et al., 2008; Maier et al., 2009)

Association was replicated and the signal was further fine mapped with the specialised SNP chip, Illumina Infinum 200K ImmunoChip, a custom ImmunoChip SNP array
of $195806$ SNPs with dense coverage in immunologically important loci designed for deep-replication and fine-mapping of 12 common auto-immune diseases.

%such as the HLA region 
There is evidence for a second independent effect from HLA class I loci,
involving HLA-A and HLA-B alleles \citep{Todd:2010bl}.
Within the HLA-A and HLA-B alleles,
the functionally important HLA-Bw4/Bw6 epitope \citep{Nejentsev:2007dv} was reported which motivated \cref{chapter:kir}.

Nonethless, SNP arrays only target common variants ($MAF > 5\%$) and
we know that many regions of the genome have been neglected by SNP arrays as they are poorly mapped on the reference genome.
Such a region is KIR which will be an object of study in \cref{chapter:kir}.

%Recent estimates of ’narrow sense’ heritability from GWAS SNPs using mixed effects models have come much closer to the total heritability, and suggest that most of the missing heritability in complex traits is probably due to a multitude of SNPs contributing tiny effects below GWAS significance thresholds. Rare variants may also explain some of the heritability in families.


%\subsection*{The many faces of type 1 diabetes}

%The clinical diagnosis of T1D is insulin insufficiency.

%\paragraph{Missing heritability}
%But how much of the estimated heritability do the genetics explain?
%The narrow sense heritability of the disease (not taking into account environmental heterogeneity) tells us that we might still be missing some genetic risk factors albeit at some very low odd ratios.
%Using a linear mixed model and including all 500K SNPs from the WTCCC, \citet{Speed:2012hi} were able to explain $74\%$ heritability.
%This suggests that alot of the missing genetic heritability can be attributed to many SNPs of small effect which do not reach genomewide significance
%but cumulatively explain a sizeable portion the variance.


%Nick Cooper
%Genome wide association studies have tagged moderate numbers of implicated SNPs in a long list of complex conditions.
%Most of these signals are quite weak and only detectable in large cohorts.
%Even once detected, combining these hits into a predictive model explains only a small amount of the variance in most phenotypes, even when a condition is known to be highly heritable through twin studies.
%This shortfall was coined as ’missing heritability’, with various suggested causes including:

%rare variants not captured in GWAS arrays,
%structural variations,
%polygenic small effects,
%and gene-gene and gene-environment interactions.
%The mixed effect method [29] is able to provide an overall variance estimate that is unbiased by sample size.

%This contrasts with standard linear models, which would be vastly skewed by the huge variable-to-case ratio of the problem space.
%The mixed-effects method has been applied to seven large Wellcome Trust Case Control Consortium (WTCCC) cohorts, which include T1D. T1D is estimated to have the largest genetic contribution of any of the complex diseases studied on this scale [30]. The initial implementations [31] of the mixed effects models suffered from bias due to the structure of LD. Speed et al [32] have improved upon the precision of these heritability estimates by modifying the matrix of relatedness between samples according to local LD, and have developed open source software to run these analyses efficiently (LDAK). Table 9 [32] below shows the weighted estimates of narrow sense heritability for 7 complex diseases.
%Using a mixed effects method I will seek to examine whether early diagnosis T1D is more or less heritable than late diagnosis T1D, and furthermore, when this variance is partitioned into chromosomes whether there is a distinction in which parts of the genome are involved at different onset-ages.

\subsection{The immune cell mechanisms}

%Genetic risk variants which are significantly associated with T1D have been identified by comparing their frequency in cases and in suitably matched controls.
%The next step is to understand why these variants are more frequent in cases.  
%However between genetic variation (e.g SNPs or copy number variation) and the disease, lies a convoluted network of regulatory genes, RNA, protein expression and environmental factors, so that all but the strongest effects might be obfuscated.  
Many of these T1D-associated genetic variants, are located in proximity to genes and regions with know immune function such as HLA and \gene{IL2RA},
which code for receptors found at the surface of immune cells.

Immune cells are white blood cells formally known as leukocytes, which agglomerate in the lymph nodes but can also be found at small concentrations in the peripheral blood.
They include lymphocytes, monocytes and granulocytes, and within these subsets, there exists a huge diversity in terms of size, gene expression and function.
It is this diversity that enables the versatility of the immune system in neutralising all kinds of pathogens (innate immunity),
its ability to distinguish them from endogenous cells (self tolerance),
%The purpose of the immune system is to distinguish self from non-self and neutralise any pathogens which enter the body.
and its capacity to adapt to better counter future infections (adaptive immunity).
An important type of leukocyte in the adaptive immune response are T lymphocytes also known as T cells.
%the cell-mediated immune response.
After having undergone central selection in the thymus,
T cells in the peripheral blood have an affinity for foreign antigens but are tolerant to self.
Initally these cells are in a naive state (naive T cells) until presented with an antigen,
at which point they mainly differentiate into effector T cells, capable of mounting an immediate response,
but also into longer-lived memory T cells, capable of mounting a stronger and faster response in the future thus
resulting in long lasting immunity againt this pathogen (acquired immunity).
In order to moderate the scale of the immune response and preserve self-tolerance, some T cells also have a regulatory function on the immune response
mediated using small signalling molecules known as cytokines (for example \cytokine{IL-2}).
These regulatory T cells are important in preventing auto-immunity and hence are the object of intense scrutiny in T1D.

Some insight into the aetiology of T1D may be gained by seeing how T1D associated variants correlate with quantitative cell phenotypes such as, 
ratios of different cell types or mean expression of surface proteins.
For example, \citet{Dendrou:2009dv} showed that, T1D risk variants risk loci which in the proximity of the protein coding gene \gene{IL2RA},
correlate with decreased mean expression of CD25 on the surface of memory T cells.
%This could be the mechanism by which T1D risk is increased since lower CD25 on memory T cells could reduce the immune tolerance.
Understanding the influence of these genetic variants on intermediate cell phenotypes can shed light on the cell mechansism which lead to T1D.
%Genetic association works on quite coarse and fuzzy clinical classification which does not account for the biological heterogeneity within cases.


%The peripheral blood is predominantly consituted of (99.9\%) red blood cells, erythrocytes and thrombocytes.
%red blood cells which bind oxygen
%Leukocytes fight off pathogens and are much rarer in the peripheral blood, as they only constitute the remaining 0.1\% \citep{Murphy:2007tl}.
%(put reference here).


%When testing for association with multiple traits, certain of these traits might be correlated so independent association testing will have lower power.
%inflate false positives (exaggerate relatedness of traits) in the same way that testing for colocolisation with shared controls exaggerates relatedness of diseases.

%but too much suppression can lead to immune deficieny.
%The balance between auto-immunity and immune deficiency is known as homeostatis.

%However too much immune suppression makes the body vulnerable
%A breakdown of self-tolerance leads to autoimmunity.

%There is a fine balance between autoimmunity and immune-deficieny

%A naive T cell (Th0 cell) is a T cell that has differentiated in bone marrow,
%and successfully undergone the positive and negative processes of central selection in the thymus.
%A naive T cell is considered mature and unlike activated T cells or memory T cells it has not encountered its cognate antigen within the periphery.
%Memory T cells are a subset of infection fighting cells that have previously encountered and responded to their cognate antigen
%Such T cells can recognize foreign invaders, such as bacteria or viruses, as well as cancer cells.
%Memory T cells have become experienced by having encountered antigen during a prior infection, encounter with cancer, or previous vaccination.
%At a second encounter with the invader, memory T cells can reproduce to mount a faster and stronger immune response than the first time the immune system responded to the invader.
%This behaviour is utilized in T lymphocyte proliferation assays, which can reveal exposure to specific antigens.


\section{Studying the immune system with flow cytometry}

%\subsection{A Brief Introduction to Flow Cytometry}

%The readings of a flow cytometer are scatter and fluorescence intensities in each channel per event recorded.
%Fluorescence intensities tend to scale multiplicatively so a logarithm or power transform is normally required to linearise the data.
%There are subsets of events which are distinguishable becaue they are more alike to each other than to the rest of the population.
%These subsets are qualified as cell populations.

%\paragraph{Distinguishing Types of Cells with Fluorescence Markers}
%Under a light microscope, leukocytes which vary from 8 to 14 micrometers in size can be coarsely classified as either granular or non-granular.
%It is also possible to distinguish with dye staining techniques (for example hematoxylin and eosin) from the shape of the nuclei, monocytes, basophiles and neutrophiles.
%However dyes do not stain individual proteins but organelles and membranes.
%and do not fluoresce.
%limitations of microscopy
%The visible spectrum is limited in terms of wavelength. This limits the resolution at which we can observe.
%Higher sensitivity of light intensity measurement is achievable than possible with naked eye.
%Outside of the visible spectrum

%\paragraph{Flow Cytometry: High-throughput detection of fluorescently labelled cell markers}
%Through the principle of fluorescence it is possible to identify specific molecules which are characterisitic to certain cells.
%Confocal Laser Scanning Microscopy for instance is a technique which can examine different layers in the cell at different confocal lenghts.


The established high-throughput method for measuring immune cell phenotypes is flow cytometry.
By labelling cells with fluorescent probes conjugated to antibodies, it is possible to distinguish a wealth of distinct cell
subsets which concomitantly express specific molecules.
Flow cytometry allows us to identify and quantify different types of cells, through individual cell measurements.

Fluorescence intensity is measured accurately by using photosensitive detectors, normally a photomultiplier tube (PMT), which turn light into an analogue (current or voltage)
or digital (photon counting) electronic signal which is later translated into a digital number indicating the intensity of the fluorescence \citep{Shapiro:2003vq,Snow:2004ci}.
For a fluorochrome to emit fluorescent light, it needs to have absorbed high energy light of a given wavelength from an illumination source, usually from a laser, which it can then release at a lower energy, longer wavelength, resulting in a so-called Stokes shift.
The wavelength spectrum at which a given fluorochrome most efficiently absorbs and emits light and Stokes shift are known and depend on the physico-chemical properties of that molecule.
To enable optical illumination, separation and collection of various fluorochromes with different emission and excitation spectra, a flow cytometer is usally equipped with serveral lasers which emit at different wavelengths and specially configured optical mirror, filters and photosensitive detectors which are sensitive to light at distinct frequency ranges \citep{Shapiro:2003vq}.

%\paragraph{Preparing and running a sample on the flow cytometer}
%\paragraph{Sample Preparation} 

When staining a sample, fluorochromes are conjugated with antibodies with an affinity for the target polypeptide we wish to quantify.
The target can be external, for example a cell receptor, or internal, for example a transcription factor or a cytokine.
If the target is internal, the cells have to undergo permeabilisation which can deteriorate the general quality of the staining.
Fluorochromes should be selected to minimise overlapping of their emission spectra.  Spectral overlap, also known as spillover, leads to a convoluted signal
signal reaching the detectors.
Antibodies, are also a potential source of noise, since both primary and secondary antibodies may bind to more than one target.
Antibodies differing in the constant regions of the heavy and light chains, known as isotypes, or non-immune sera, can be used to control non-specific staning and/or reduce non-specific binding by blocking secondary targets.
%Secondary antibodies known as isotypes can be used to reduce non-specific binding by blocking secondary targets.

%\paragraph{Running a Sample on the Flow Cytometer}
Once a solution of fluorescently labeled cells is fed to the flow cytometer,
the sample is delivered to the flow cell after hydrodynamic focusing.
In the flow cell, the cells ared filed up individually so they cross a laser beam one by one \citep{Shapiro:2003vq}
%the fluidic system of the instrument separates the cells, under hydrodynamic pressure, and files them up individually so that they cross a laser beam one by one \citep{Shapiro:2003vq}.
As a cell crosses the laser beam some light is scattered and some is absorbed. The detected scattered light is used to provide an estimate of the size and granularity of the cell.
Light scattered in the forward direction (diffracted light) is correlated with the size of the cell, whereas light scattered sideways (refracted light) is correlated with the complexity of the cellular structure.
The absorbed light is later emitted as heat and fluorescent light.
The intensity of the scattered and fluorescently emitted light measured by the detector thus provides quantitative information about the correlates of size and granularity, and the presence of certain fluorescently-marked molecules for each cell.
When examining leukocytes, using only the physical properties provided by the scattered light intensity it is possible to distinguish lymphocytes from monocytes and more granular neutrophils.
Combining this information with the fluorescent intensities it is possible to further distinguish between different types of lymphocytes which have in common certain cellular receptors or transcription factors.

%\subsection{What is Flow Cytometry, How Does it Work?}
%First the sample is prepared.
%Cells are labelled with various antibodies conjugated with different fluorescent probes which bind to specific targets inside or on the surface of the cell with the aim
%of uniquely labelling different cellular markers.
%The sample is then fed to the flow cytometry instrument.
%The cells in the sample are hydro-dynamically focused so that they file up as they go through one or more lasers.
%The purpose of the lasers is to excite the fluorescent probes attached to the cell which then let off a fluorescent signal whose intensity
%is captured by a number of receptors.
%Different receptors are designed to measure different ranges of intensities.
%The strength of the signal correlates with the number of fluorescent probes which are attached to the target.
%Another type of measure which is captured is how the light from the laser scatters as it hits the cell.
%Side scatter tends to correlate with the density of the cell whereas forward scatter tends to correlate with the size of the cell.
%In practice however many complications can occur during the preparation and running of the sample.
%Cells may clump together, antibodies may be none specific, cells die causing cell debris, fluorochromes can deteriorate and not fluoresce in the expected spectrum.
%All these factors tend to result in erratic or misleading fluorescence reading which contribute to making the results noisy and hard to analyse.

Flow data has many intricacies
such as compensation to account for fluorescence crosstalk (\Cref{appendix:compensation}),
data format (\Cref{appendix:fcs-data-format})
and choice of transformation (\Cref{appendix:transformation}),
which need to be understood before attempting to extract information from the data.

%\paragraph{Noise} 
Futhermore, there are many sources of noise in flow cytometry which complicate sample comparison and impact reproducibility.  
Sample processing and storage conditions noise:
certain surface markers are more fragile than others and may be shed when cells are frozen.
Depending on the day of analysis, samples might look remarkably different.
This is why it is preferable to analyse samples on the day.  
Noise associated with the staining of the sample:
the qualitative and quantitative choices for selecting antibodies and fluorochrome determine the quality of the staining.
Antibodies have a tendency to be sticky and can bind to other targets or in an erratic manner leading to spurious signal.
The level of non-specific binding can be assessed with isotype antibodies.  
Noise linked to the instrument:
the reliability of the lasers and detectors may wither with time.
Fluorescent beads can be used to detect and correct these variations.  
Noise due to the flow operator:
sometimes the operator might decide to not collect all the events and for example apply a cutoff.

All these sources of noise contribute to different patterns of staining and concentrations of debris
which can lead to spurious cell populations or skew the analysis, complicating the analysis across samples.
Some of these issues are addressed by the \gls{HIP} consortium standardisation efforts \citep{Maecker:2012gl}
by using lyoplates, experimental protocols and instrument configurations.
However other issues need to be dealt with using data analysis techniques such as normalisation.

\section{Normalisation}
%\section{Normalisation: why it is needed, methods, utility}

The purpose of normalisation is to remove unwanted experimental variation to make data comparable even when the samples are
collected on different days, processed with different protocols or instrumental configurations.
However distinguishing between unwanted and biological variation necessitates some prior knowledge about the datasets, either in the form of distributional assumptions
or in the form of features which exist in a predictiable relationship across samples.
Such features can then be used as reference points to normalise across samples.
If the features are modes in the data, then normalisation is equivalent to doing clustering, in order to identify the modes,
followed by meta-clustering to match the modes across samples.
In microarray gene expression datasets, for example, one distributional assumption is that the majority of genes are not differentially expressed between samples from
similar tissue types.
The underlying principle is that true biological variation is specific whereas experimental variation affects the sample as a whole.
In order to decouple technical variation from biological variation, without making assumptions about the distributions of the data,
synthetic object of known and stable property, such as fluorescent beads in flow cytometry, can be used to this purpose.


%\paragraph{Instrument variation}

\section{Clustering}

%\subsection{Analysis of Results: Identifying Cell Populations and Defining Cell Phenotypes}
%\subsection{Analysis of Flow Data: Identifying Cell Populations}

Once the data has been made comparable thanks to normalisation, the next task is to identify clusters, groups of cells of similar of properties,
which can be matched across samples.
%, either pooling the samples together or by clustering them separately.

%Once the data is normalised, we can pool across samples to proceed with cluster analysis, the purpose of which is to identify homogeneous (sharing the same properties) groups (clusters) in the data.
%The task is to define groups in such a way that data points in the same group are more similar, in some sense or another, to each other than to those in other groups (clusters).

%Clustering is the main task of exploratory and statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, and bioinformatics.
%Clustering itself is not one specific algorithm, but the general task to be solved.
%It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them.
%Popular notions of clusters are:
%\begin{itemize}
  %\item groups with small distances among the cluster members
  %\item dense areas of the data space
  %\item mixtures of particular statistical distributions
%\end{itemize}
%The appropriate clustering algorithm and parameter settings (including values such as the distance function to use, a density threshold or the number of expected clusters)
%depend on the individual data set and intended use of the results.
%

%\paragraph{Identifying Cell Populations with Manual Gating}
\subsection{Manual clustering}

Given a one, two or three dimensional projection of the data, clusters can in some cases, be identified by eye.
In the context of flow cytometry, clusters constitute cell populations identifiable by the concomitant expression of internal or surface markers.
When the properties of the sought population are known, a manual or supervised (semi automatic) approach can be used.

The manual method, known as manual gating,
is a step-by-step method where we consider and plot two channels at a time and delineate a region, called a gate, such that cells which lie outside the gate are filtered out.
%This step is repeated for a required number of iterations.
The result is that a population is defined as an intersection of multiple one or two dimensional gates.
This process leaves room for improvement due to poor scaling when we increase the number of parameters (fluorochromes in flow cytometry).
%hence the number of dimensions in the data
%The inability to cluster in more than three dimensions at a time also
Also only the pairwise correlation can be assessed which impinges
on identification of higher-dimensional clusters.
%since information is lost at each clustering step
%Only methods which are capable of clustering all dimensions at the same time can exploit the full information available in the dataset.
The ordering in which the gates are drawn thus has some bearing on the final clustering solution.

More importantly this step introduces further non-biological variation to an already noisy data set since the position of the gates
on the same data can differ between gaters \citep{Maecker:2010fg}.
%This provides motivation for considering more efficient and unbiased alternative methods of doing the gating.
It also suffers from strong bias as it tends to force data to fit a model (the gater's expectation).
Finally manual gating is not feasible when an exhaustive enumeration of all identifiable cell populations is required \citep{Siebert:2010iv,Aghaeepour:2012fq} especially
as the number of cellular markers increases.
For this, unsupervised computational methods, which do not rely on visualisation, are essential.


\subsection{Automatic methods for identifying clusters}

Automatic flow data analysis methods were first reviewed in \citeyear{Bashashati:2009em} by \citeauthor{Bashashati:2009em} then in \citeyear{Lugli:2010ki} by \citeauthor{Lugli:2010ki}
and more recently by \citet{Aghaeepour:2013dg}.
%\citeauthor{Lugli:2010ki} suggest looking at NMF.
They are benchmarked annually by the FlowCAP (Flow Cytometry Critical Assessment of Population Identification Methods)
group and broadly fall in two camps:
%The outcome of FlowCAP is that ensemble methods (boosting approaches) might be the way forward.  
%cell population identification methods
unsupervised methods which have have unlabelled data
and
%sample classification methods
supervised methods which require manual training by giving approximate starting gates.

Clustering methods which make explicit assumptions about the shapes of populations are model-based or parametric.
Methods which do not are said to be heuristic or non-parametric,
although the latter can be limiting cases of parametric models (\Cref{appendix:clustering}).
Here I will mostly review unsupervised methods where training data is not provided,
and focus on those which require specification of only some parameters such as the expected
number of clusters.

\paragraph{Model based methods: low variance high bias methods}

Model based methods assume that the flow data can be explained by a mixture of multivariate distributions where each distribution is representative of a different type of cell.
A nice feature of these methods is that they can assign a probability of population membership to each cell which can be exploited in downstream statistical analysis to account for uncertainty in the clustering.
The first and simplest of these methods assumed cell populations could be represented by a mixture of multivariate Gaussians \citep{Chan:2008gq}.
However this method tends to over estimate the number of multivariate Gaussians which best models the data since outliers which are in the tails of the distributions
are explained by new low mixture distributions, increasing unreasonably the number of cell populations.
To account for this, Flowclust \citep{Lo:2008it} replaced Gaussians by more robust heavy tailed t-distribution.
However t-distributions are still symmetric distributions and so can not represent skewed populations which are common in stimulation experiments or more generally when cells are in a transitional state from one cell type to another.
\citet{Pyne:2009hl} addressed this issue in FLAME (Flow Analysis with Automated Multivariate Estimation) by employing skewed t-distributions instead.
Yet a remaining issue is that these distributions are convex by nature and so a concave population which can arise in transitional cell populations undergoing progressive change on more than one marker may only be represented by the merging of several convex populations.
This merging step can be accomplished using FlowMerge \citep{Finak:2009fk}.
Finally the main drawback with these model-based methods are that the estimation of the parameters of these distributions (mean, central location, covariance, skewness factor, degrees of freedom) are computationally intensive and that the algorithms to calculate those scale poorly to multiple dimensions (EM \citep{Dempster:1977ul} and MCMC) and are sensitive to starting conditions.
%low variance high bias

\paragraph{Non-model based methods or non-parametric methods: low bias high variance methods}

Non model-based methods make no explicit assumptions about the shape of populations but instead try to minimise some loss function.
%These methods do not return any explicit parameters.
They use measures of density or distance to identify clusters of points.
%Distance is a more suitable metric when $N \ll P$ and density when $N \gg P$.
However certain non-parametric methods turn out to be limiting cases of parametric methods.
For example K-means is a limiting case of spherical Gaussian Mixture Models when the covariance tends to zero (\Cref{appendix:clustering}),
because maximising the log-likelihood is asymptotically equivalent to minimising the residual sum of squares.

%However when spillover occurs the correlation has no biological meaning but the correlation can still be used to identify clusters.
%The correlation within clusters is different than the overall correlation.
%of tightly connected points.
There are numerous clustering methods which rely on density estimation.
Possibly the most famous is Density Based Merging (DBM) which uses a region growing density approach so that points of high density are linked \cite{Walther:2009gn}).
Flow Clustering without K (FLOCK) \citep{Qian:2010ep} relies on grid based density estimation in higher dimensions.
Misty Mountain \citep{Sugar:2010jf} takes decreasing cross sections of the two dimensional density histogram so that clusters are progressively merged as the threshold decreases.
This approach can not distinguish rare cell population in close proximity to bigger cell populations and is limited to two dimensions.
CurvHDR (negative Curvature and High Density Region, \citet{Naumann:2010fp}) identifies regions of significant curvature in the multivariate density function,
by using a bandwidth rather than the number of bins.
However this approach does not go beyond four dimensions.
All these density based approaches require the number of bins or the bandwidth to be specified by the user or estimated from the data according to some heuristic.
Care needs to be taken when selecting an appropriate bin or bandwidth as small number of bins or larger bandwidths,
lead to overestimation of the density in low density regions and underestimation in high density regions.
%On the other hand in the bandwidth is too small then the dynamic range of the density becomes small (few density levels).
However as the number of dimensions increases, data tends to become sparser and distance based clustering becomes more appropriate.
%Unfortunately, many distance based clustering methods rely on computation of the entire distance matrix, provided the number of data points is not
K-means is probably the simplest and most famous of these methods
since it does not rely on the computation of the entire distance matrix (\Cref{appendix:clustering}).
%Given an initial set of cluster means, which may be user-defined or selected at random, points are then assigned to the cluster.
%After the assignment, the cluster centers are recomputed, and again points are re-assigned to whichever cluster ce
For example flowMeans \citep{Aghaeepour:2010fv} is an extension to K-means for flow cytometry.
However K-means is known to be very sensitive to starting conditions and is also not robust to outliers.
%a high variance method because small changes in the data can lead to very different clustering solutions.
Methods which calculate the entire distance matrix require some form of downsampling to reduce the total number of points.
%$N$ when $N$ is large.
Caution needs to be taken since random uniform downsampling runs the risk of discarding rarer populations.
One workaround is to account for local density when doing the downsampling so that data is not lost.
Once the downsampling has sufficiently reduced the number of points so that distance matrix may be computed,
methods such as, spectral clustering which uses spectral graph theory to determine where to partition the network
Sampling followed by Spectral Clustering (SAMSpectral, \citet{Zare:2010cw}) or more conventional hierarchical clustering
as done by Spanning-tree Progression Analysis of Density-normalized Events (SPADE, \citet{Simonds:2011jh}), may be applied.

The advantage of these non-parametric methods is that they are fast and flexible and can often come with heuristics for estimating the parameters.
Unfortunately the disadvantage is that the results are not easily interpretable or generalisable to another dataset since there is no explicit model.
%These methods return a clustering of the data but do no estimate any parameters from the data.
They are also sensitive to starting conditions and tuning parameters which greatly complicates comparison of clustering results across datasets.
%Also they tend to rely on user-specified parameters which have no biological significance.
%Moreover they do hard clustering 
%high variance low bias


\paragraph{Estimating the optimal number of clusters from the data}
%\paragraph{Estimating the optimal number of clusters: avoid overfitting}

%The methods described above return the optimal cluster assignment given an expected number of clusters.
The methods described above, normally require the expected number of clusters as input parameter.
Although there are many suggested heuristics,
estimating an optimal number of clusters from the data remains an open problem in machine learning and statistics.
%However, estimating an optimal number of clusters from the data,
%or more generally the number of parameters in a model

%The model which best explains the data i.e minimises the mean square error is the saturated one which assigns each cell to its own cluster.
%Clearly this is a uninformative and trivial interpretation of the data since it tells us nothing new and holds no predictive power.
%This phenomenon is known as overfitting.

%Automatic gating is essentially a clustering problem when the number of clusters may not be known.  

%The criterion can be to maximise information.
Generally these methods seek to maximise utility by reaching a compromise between model complexity, number of parametes in the model,
and accuracy, in supervised clustering, or some other metric like variance explained, in unsupervised clustering.
%Model complexity is penalised by the number of parameters added to the model.
%limiting the variance explained by each cluster using shrinkage methods,
%or by penalising for the number of parameters added to the model.
%Shrinkage can be useful when parameters are colinear.
%because effectively each parameter is reducing the degrees of freedom.
%These can be thought of different methods of parameter regularization.

One way of estimating the expected number of clusters, is to find modes in the data: regions of significantly high density \citep{Duong:2008eu,JING:2012ek}.
Another way is to look at the ratio of intra-cluster to inter-cluster variance at increasing number of clusters and pick the elbow point where the ratio drops significantly.
This relates to the $R^2$ ratio in \gls{ANOVA}, since the intercluster variance is the variance explained by the model and the intracluster variance is the remaining variance.
\[
  R^2 = 1 - \frac{ \sum}
\]

Another approach it to maximise the Bayesian Information Criterion, the log of the likelihood penalised by the number of clusters and free parameters.
%In the non model based methods, a threshold for the inter cluster distance defines the number of clusters

%Another solution to the overfitting problem, is to introduce an extra parameter $\lambda$ which weights the influence of the variables in the model.
%Like with clustering there's a hard approach and a soft approach.
%If the weights are either 0 or 1, then variables are either excluded or included in the the model, this is the variable selection method.
%If $\lambda$ is allowed to take a range of values between 0 and 1 then this is the shrinkage methods such as ridge-regression and lasso.

An interesting alternative to the number of cluster estimation problem is a derivative of the k-means algorithms,
the Dirichlet Process means, which includes the number of clusters
K as a parameter to be estimated from the data.
By taking larger subsets of the data, new clusters are created.

%However, these clusters often need to be matched across samples for association testing.

%\paragraph{Shrinkage estimation}
%
%In the case where $N \ll P$, estimation of the covariance and correlation are very unstable.
%the covariance matrix becomes singular, i.e. it cannot be inverted to compute the precision matrix.  
%As an alternative, many methods have been suggested to improve the estimation of the covariance matrix.
%All of these approaches rely on the concept of shrinkage.
%This is implicit in Bayesian methods and in penalized maximum likelihood methods and explicit in the Stein-type shrinkage approach.  
%A simple version of a shrinkage estimator of the covariance matrix is constructed as follows.
%One considers a convex combination of the empirical estimator (A) with some suitable chosen target (B), e.g., the diagonal matrix.
%Subsequently, the mixing parameter ($\delta$) is selected to maximize the expected accuracy of the shrunken estimator.
%This can be done by cross-validation, or by using an analytic estimate of the shrinkage intensity.
%The resulting regularized estimator ($\delta$ A + (1 - $\delta$) B) can be shown to outperform the maximum likelihood estimator for small samples.
%For large samples, the shrinkage intensity will reduce to zero, hence in this case the shrinkage estimator will be identical to the empirical estimator.
%Apart from increased efficiency the shrinkage estimate has the additional advantage that it is always positive definite and well conditioned.
%A review on this topic is given, e.g., in Schäfer and Strimmer 2005.  
%A covariance shrinkage estimator is implemented in the R package "corpcor" and the scikit-learn library for the Python (programming language).
%


%\paragraph{Cell phenotypes: cluster properties}
\paragraph{Association testing}

Provided clusters have been identified and matched across samples within a study, it is then possible to test for association of cell population
features with genotype, sex, age or clinical outcome.
Two examples of such population features are the size of the population over its parent population and the \gls{MFI} on a particular marker.
The fluorescence intensity correlates with the quantitative expression of a protein on the cell surface.
These cell population features are commonly referred to as cell phenotypes.  
When assessing the gene to phenotype correlation, the reproducibility of these cell phenotypes is crucial since
larger within-individual variance decreases the power to detect between-individual effects.
%Certain phenotypes are more stable than others.

As we will see in \Cref{chapter:il2ra}, the method used to do the gating of the cell phenotypes can have an important impact on the association
statistics.

%When a large number of clusters has been identified in a small number of samples then we risk overfitting.
%To avoid overfitting feature selection
%There has been work undertaken to select the features which best correlate with genotype.
%FeaLect and RchyOptimyx constructs a hierarchy of cells which best explains clinical outcome or cytokine response
%
%For each feature, a score is computed that can be useful for feature selection.
%Several random subsets are sampled from the input data and for each random subset, various linear models are fitted using lars method.
%A score is assigned to each feature based on the tendency of LASSO in including that feature in the models.
%Finally, the average score and the models are returned as the output.
%
%The features with relatively low scores are recommended to be ignored because they can lead to overfitting of the model to the training data.
%Moreover, for each random subset, the best set of features in terms of global error is returned.
%They are useful for applying Bolasso, the alternative feature selection method that recommends the intersection of features subsets.
%
%Another issue is because all features are derived from the same sample, traits might be highly correlated and so colinear.
%
%The approach followed by Nima et al, was to identify in an unsupervised manner cell features which correlate well with genotype or clinical outcome.
%Cell phenotypes from more general cell populations were chosen
%


%\paragraph{ Distance is a more suitable metric when  and density when $N \gg P$. }

%wikipedia


%\paragraph{Density Estimation}
%
%Density estimation is the problem of modeling a density given a finite number of data points drawn from that density function.
%Clustering can be thought of as a special case of density estimation: measuring the class-conditional density.
%There are two basic approaches to density estimation.
%Parametric where a given form for the density function is assumed (e.g Gaussian) and the parameters are optimised to maximise the likelihood.
%Non-parametric where no functional form for the density function is assumed and instead the density estimation is driven entirely by the data.
%These methods require such as the bin width for crude density estimation using the histogram, and the kernel bandwidth and the choice of kernel function in the kernel density estimation. A major drawback of the histogram approach are the location of the bins.
%Chosing an appropriate parameter relies on some heuristic and the choice of parameter is particular sensitive to the size and dimensions of the dataset.
%
%The median distance from the origin to the closest point where $N$ is the number of points and $p$ is the number of dimensions is given by (accord to Elements of Statistical Learning):
%% figure for Elements of Statistical Learning
%\[
  %d(p, N) = \left( 1 - \frac{1}{2}^{1/N} \right)^{1/p}
%\]
%
%
%Another technique is to use the distance matrix to the K nearest neighbours of each point in order to compute the local density,
%since density is inversely proportional to distance.
%Points in sparser areas will have a larger maximum distance to their Kth nearest neighbour than points in denser areas.
%This is the K nearest neighbour approach for which a very implementation exists provided the data is stored using a kd-tree or a locality hashing function.
%This permits very fast lookup of nearest neighbours and is called the Approximate Nearest Neighbour method.
%

%\subsection{Cross sample comparison without gating}
%If the objective is simply to classify samples or simply do cross sample comparison
%then gating is not necessary and the flow data can be treated as a whole
%and projected to lower dimensional space using PCA \citep{Costa:2010cq}.
%We can also extract a signature or finger print \cite{Rogers:2008ij}\cite{Rogers:2009jz}.
%Of course this method is dependent on how the features are selected for creating the signature.
%Probability binning can be used for this \cite{Roederer:2001tz}  whereby the data is binned in grid.
%The idea is then to look for samples which are significantly different when compared to control samples.

%In this project, the cell subsets we wished to identify are known so it was considered better to develop our own method which relies on partially manually gated data to make sure we were gating the right subsets.
%
